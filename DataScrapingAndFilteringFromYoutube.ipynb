{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dH78wYL_5C7",
        "outputId": "3510f18a-f997-4715-ace2-7a06f7465405"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk import pos_tag\n",
        "from nltk import ne_chunk\n",
        "import regex as re\n",
        "import spacy # We need spacy for german lemmatization\n",
        "#import de_core_news_sm\n",
        "import en_core_web_sm\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "from transformers import pipeline\n",
        "from transformers import DistilBertTokenizer\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "#from germansentiment import SentimentModel\n",
        "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from googleapiclient.discovery import build\n",
        "import pandas as pd\n",
        "import getpass\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "K63x6ylOtJmR"
      },
      "outputs": [],
      "source": [
        "########### ENTER YOUR API KEY HERE ###########\n",
        "\n",
        "api_key = 'AIzaSyA7fWeIgqdlPRzHhKlRVtoN48tDgi_oJQY'\n",
        "\n",
        "########### ENTER THE PLAYLIST ID HERE ###########\n",
        "\n",
        "playlist_ids = ['PLLtT6fiQ1SOObtxQvrB6ekyBEwV6NbHHK']\n",
        "\n",
        "\n",
        "########### ENTER THE EXPORT LOCATION HERE ###########\n",
        "\n",
        "csv_file = '/content/english_neutral_bias_test.csv'  # Name your file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "DDYx6qk2tP5Y"
      },
      "outputs": [],
      "source": [
        "# Build the YouTube client\n",
        "youtube = build('youtube', 'v3', developerKey=api_key)\n",
        "\n",
        "def get_all_video_ids_from_playlists(youtube, playlist_ids):\n",
        "    all_videos = []  # Initialize a single list to hold all video IDs\n",
        "\n",
        "    for playlist_id in playlist_ids:\n",
        "        next_page_token = None\n",
        "\n",
        "        # Fetch videos from the current playlist\n",
        "        while True:\n",
        "            playlist_request = youtube.playlistItems().list(\n",
        "                part='contentDetails',\n",
        "                playlistId=playlist_id,\n",
        "                maxResults=50,\n",
        "                pageToken=next_page_token)\n",
        "            playlist_response = playlist_request.execute()\n",
        "\n",
        "            all_videos += [item['contentDetails']['videoId'] for item in playlist_response['items']]\n",
        "\n",
        "            next_page_token = playlist_response.get('nextPageToken')\n",
        "\n",
        "            if next_page_token is None:\n",
        "                break\n",
        "\n",
        "    return all_videos\n",
        "\n",
        "# Fetch all video IDs from the specified playlists\n",
        "video_ids = get_all_video_ids_from_playlists(youtube, playlist_ids)\n",
        "\n",
        "\n",
        "\n",
        "# Function to get replies for a specific comment\n",
        "def get_replies(youtube, parent_id, video_id):  # Added video_id as an argument\n",
        "    replies = []\n",
        "    next_page_token = None\n",
        "\n",
        "    while True:\n",
        "        reply_request = youtube.comments().list(\n",
        "            part=\"snippet\",\n",
        "            parentId=parent_id,\n",
        "            textFormat=\"plainText\",\n",
        "            maxResults=100,\n",
        "            pageToken=next_page_token\n",
        "        )\n",
        "        reply_response = reply_request.execute()\n",
        "\n",
        "        for item in reply_response['items']:\n",
        "            comment = item['snippet']\n",
        "            replies.append({\n",
        "                'Timestamp': comment['publishedAt'],\n",
        "                'Username': comment['authorDisplayName'],\n",
        "                'VideoID': video_id,\n",
        "                'Comment': comment['textDisplay'],\n",
        "                'Date': comment['updatedAt'] if 'updatedAt' in comment else comment['publishedAt']\n",
        "            })\n",
        "\n",
        "        next_page_token = reply_response.get('nextPageToken')\n",
        "        if not next_page_token:\n",
        "            break\n",
        "\n",
        "    return replies\n",
        "\n",
        "# Function to get all comments (including replies) for a single video\n",
        "def get_comments_for_video(youtube, video_id):\n",
        "    all_comments = []\n",
        "    next_page_token = None\n",
        "\n",
        "    while True:\n",
        "        comment_request = youtube.commentThreads().list(\n",
        "            part=\"snippet\",\n",
        "            videoId=video_id,\n",
        "            pageToken=next_page_token,\n",
        "            textFormat=\"plainText\",\n",
        "            maxResults=100\n",
        "        )\n",
        "        comment_response = comment_request.execute()\n",
        "\n",
        "        for item in comment_response['items']:\n",
        "            top_comment = item['snippet']['topLevelComment']['snippet']\n",
        "            all_comments.append({\n",
        "                'Timestamp': top_comment['publishedAt'],\n",
        "                'Username': top_comment['authorDisplayName'],\n",
        "                'VideoID': video_id,  # Directly using video_id from function parameter\n",
        "                'Comment': top_comment['textDisplay'],\n",
        "                'Date': top_comment['updatedAt'] if 'updatedAt' in top_comment else top_comment['publishedAt']\n",
        "            })\n",
        "\n",
        "            # Fetch replies if there are any\n",
        "            if item['snippet']['totalReplyCount'] > 0:\n",
        "                all_comments.extend(get_replies(youtube, item['snippet']['topLevelComment']['id'], video_id))\n",
        "\n",
        "        next_page_token = comment_response.get('nextPageToken')\n",
        "        if not next_page_token:\n",
        "            break\n",
        "\n",
        "    return all_comments\n",
        "\n",
        "# List to hold all comments from all videos\n",
        "all_comments = []\n",
        "\n",
        "\n",
        "for video_id in video_ids:\n",
        "    video_comments = get_comments_for_video(youtube, video_id)\n",
        "    all_comments.extend(video_comments)\n",
        "\n",
        "# Create DataFrame\n",
        "comments_df = pd.DataFrame(all_comments)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "z_J8QsqMthUQ"
      },
      "outputs": [],
      "source": [
        "########### ENTER EXPORT LOCATION HERE ###########\n",
        "\n",
        "# Export whole dataset to the local machine as CSV File\n",
        "comments_df.to_csv(csv_file, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "ct5p778A_xQY"
      },
      "outputs": [],
      "source": [
        "def remove_emoji(comment):\n",
        "    \"\"\"Function to remove emojis.\n",
        "        comment : data input ; str\n",
        "        Taken from :\n",
        "        https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
        "                               u\"\\U00002702-\\U000027B0\"\n",
        "                               u\"\\U00002702-\\U000027B0\"\n",
        "                               u\"\\U000024C2-\\U0001F251\"\n",
        "                               u\"\\U0001f926-\\U0001f937\"\n",
        "                               u\"\\U00010000-\\U0010ffff\"\n",
        "                               u\"\\u2640-\\u2642\"\n",
        "                               u\"\\u2600-\\u2B55\"\n",
        "                               u\"\\u200d\"\n",
        "                               u\"\\u23cf\"\n",
        "                               u\"\\u23e9\"\n",
        "                               u\"\\u231a\"\n",
        "                               u\"\\ufe0f\"  # dingbats\n",
        "                               u\"\\u3030\"\n",
        "                               \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', comment)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "sP0E8avz_2SA"
      },
      "outputs": [],
      "source": [
        "def P_data_reading(path):\n",
        "    \"\"\"Simple function to read in the data we want to use.\n",
        "       path : the path pointing to our data ; csv file\n",
        "    \"\"\"\n",
        "\n",
        "    comments_data = pd.read_csv(path)\n",
        "\n",
        "    ############### FOR TESTING PURPOSES, WE ONLY TAKE FIRST 20 ###############\n",
        "\n",
        "    # Turn into Series, containing only the comments\n",
        "    return comments_data['Comment']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "lAzzFmkXCpaI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "ikhPOFTZ_7l4"
      },
      "outputs": [],
      "source": [
        "def P_data_cleaning(data, language):\n",
        "    \"\"\"Function to clean our data.\n",
        "       data : data input ; pd.Series\n",
        "       language : what language the comments are in (input in lowercase) : str\n",
        "    \"\"\"\n",
        "\n",
        "    # REMOVE NAN ENTRIES\n",
        "    data = data.dropna()\n",
        "\n",
        "    # FOR GERMAN DATA : Change ö , ä , ü to oe, ae, ue\n",
        "    data = data.str.replace(\"ö\", \"oe\").str.replace(\"ä\", \"ae\").str.replace(\"ü\", \"ue\")\n",
        "\n",
        "    # REMOVING PUNCTUATION\n",
        "    data = data.str.replace('[^a-zA-Z0-9]',' ')\n",
        "\n",
        "    # REMOVING EMOJIS\n",
        "    data = data.apply(lambda x: remove_emoji(x))\n",
        "\n",
        "    # LOWERCASE\n",
        "    data = data.str.lower()\n",
        "\n",
        "    # REMOVING STOPWORDS\n",
        "    data = data.apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords.words(language))]))\n",
        "\n",
        "\n",
        "    return data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "EZ6GKG7IATdR"
      },
      "outputs": [],
      "source": [
        "def P_data_tokenization(comment, language, model):\n",
        "    \"\"\"\n",
        "    Tokenization function. We implement different tokenizers\n",
        "    comment : the current comment to analyze ; string\n",
        "    language : the language for tokenization ; string\n",
        "    model : the tokenizer we are using (or from which model we are using the tokenizer from)\n",
        "    \"\"\"\n",
        "\n",
        "    if model.lower() == 'distilbert':\n",
        "        # We use the distilBERT tokenization (in case we are going to use that model later on)\n",
        "        # NOTE : don't know what languages are included in multilingual, I just know german is in it\n",
        "        tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-multilingual-cased\")\n",
        "\n",
        "      #  encoded_comment = tokenizer.encode(comment, add_special_tokens=True)\n",
        "\n",
        "\n",
        "        # Tokenizing and padding comments (padding needed for constant input later on in distilBERT model)\n",
        "        tokenized_comment = tokenizer.encode_plus(\n",
        "            comment,\n",
        "            max_length=128,  # Set the desired maximum sequence length\n",
        "            padding='longest',  # Pad to the longest sequence in the batch\n",
        "            truncation=True,  # Truncate if needed\n",
        "            return_tensors='pt',  # Return PyTorch tensors\n",
        "            )\n",
        "\n",
        "        # Access the input IDs (we'll use these for fine-tuning (? on which data will we do fine-tuning ? Daniele proposed\n",
        "        # the english comment section for AI on youtube, because there we have so much data and it is similar to ours))\n",
        "        #input_ids = tokenized_comment['input_ids']\n",
        "\n",
        "        # Will return input ids and attention mask of our inputs\n",
        "        return tokenized_comment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "mkojlm8sAY8_"
      },
      "outputs": [],
      "source": [
        "def P_data_lemmatizing(comment, language):\n",
        "    \"\"\"FILTER APPROACH 1 : We first lemmatize so we get the base words of everything\n",
        "       + we have less words in general and can build bigger groups\n",
        "       - we will lose some accuracy in our sentiment analysis : words like best/better/good will all be just good\n",
        "\n",
        "       To combat the negative effect, we will do the following : Build a mapping between the original input sentences\n",
        "       and the lemmatized ones. We will just lemmatize to build the bigger groups and denoise our dataset. Then, when we\n",
        "       done this, we map back to the original sentences and tokenize.\n",
        "\n",
        "       Since we use pandas, we just won't reset indices. That way, we just keep the original pandas dataset (i.e. we save\n",
        "       a copy of it after the cleaning steps and right before lemmatizing) and then use the indices for our mapping.\n",
        "\n",
        "       comment : the current comment to analyze ; string\n",
        "       language : the language for tokenization ; string\n",
        "    \"\"\"\n",
        "    if language.lower() == 'german':\n",
        "        lemmatizer = spacy.load(\"de_core_news_sm\")\n",
        "\n",
        "\n",
        "    if language.lower() == 'english':\n",
        "        lemmatizer = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "\n",
        "\n",
        "    lemmatized_comment = ' '.join([token.lemma_ for token in lemmatizer(comment)])\n",
        "\n",
        "    # After lemmatizing, some words are again higher cased\n",
        "    lemmatized_comment = lemmatized_comment.lower()\n",
        "\n",
        "\n",
        "    return lemmatized_comment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "qmKV6_1TAbx8"
      },
      "outputs": [],
      "source": [
        "def P_data_word_count(data):\n",
        "    \"\"\"FILTER APPROACH 1 : We find the buzz words we want to filter for.\n",
        "       The idea is to iterate through our own data and see if there are\n",
        "       some really common words that are used for showing ones sentiment\n",
        "       if there is a pattern, we can use these to remove the noise from\n",
        "       our data\n",
        "       data : data input : pd.Series\n",
        "    \"\"\"\n",
        "    # explode() : convert each single element into a row\n",
        "    # We also sort them to find the most common ones\n",
        "    word_counts = data.str.split().explode().value_counts().sort_values(ascending = False)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # We return the count aswell as the (lemmatized) words themselves\n",
        "    return word_counts, list(word_counts.index)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "cgHekKQIAe0x"
      },
      "outputs": [],
      "source": [
        "def P_data_filtering(sentiment_words, model, language, threshold = 0.95):\n",
        "    \"\"\"FILTERING APPROACH 1 : We do pre-filtering on our data to remove noise.\n",
        "       For this, we use pre-trained, state-of-the-art models to find the sentiments of different words in different languages.\n",
        "       Next, we filter the data (see details below)\n",
        "       sentiment_words = list of words we want to use for filtering : List of String\n",
        "       model : which model to use\n",
        "       language : the language for tokenization ; string\n",
        "       threshold : threshold on the confidence level of sentiment predictions of the single words ; Float\n",
        "    \"\"\"\n",
        "\n",
        "    if language.lower() == 'english':\n",
        "        # According to :\n",
        "        # https://huggingface.co/rabindralamsal/BERTsent?text=I+like+you.+I+love+you\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\"rabindralamsal/BERTsent\")\n",
        "        model = TFAutoModelForSequenceClassification.from_pretrained(\"rabindralamsal/BERTsent\")\n",
        "\n",
        "        data = {'word' : [], 'sentiment_label' : [], 'confidence_pos' : [], 'confidence_neg' : [], 'confidence_neutral' : [], 'confidence_highest' : []}\n",
        "\n",
        "        for word in sentiment_words:\n",
        "\n",
        "            input = tokenizer.encode(word, return_tensors=\"tf\")\n",
        "            output = model.predict(input)[0]\n",
        "            prediction = tf.nn.softmax(output, axis=1).numpy()\n",
        "            sentiment = np.argmax(prediction)\n",
        "\n",
        "            # Convert sentiments (as they are stored 0,1,2 in this model)\n",
        "            if sentiment == 0:\n",
        "                sentiment = 'negative'\n",
        "            elif sentiment == 1:\n",
        "                sentiment = 'neutral'\n",
        "            elif sentiment == 2:\n",
        "                sentiment = 'positive'\n",
        "\n",
        "\n",
        "            data['word'].append(word)\n",
        "            data['sentiment_label'].append(sentiment)\n",
        "            data['confidence_pos'].append(prediction[0][2])\n",
        "            data['confidence_neg'].append(prediction[0][0])\n",
        "            data['confidence_neutral'].append(prediction[0][1])\n",
        "            data['confidence_highest'].append(max(prediction[0][0],prediction[0][1],prediction[0][2]))\n",
        "\n",
        "\n",
        "            words_sentiments_confidence = pd.DataFrame(data, columns=['word', 'sentiment_label', 'confidence_pos', 'confidence_neg', 'confidence_neutral', 'confidence_highest'])\n",
        "            words_sentiments_confidence.to_csv('/content/words_and_sentiments.csv')\n",
        "\n",
        "\n",
        "            words_sentiments_confidence_filtered = words_sentiments_confidence[(words_sentiments_confidence['confidence_highest'] >= threshold)\\\n",
        "                                                                                & (words_sentiments_confidence['confidence_highest'] != words_sentiments_confidence['confidence_neutral']) \\\n",
        "                                                                                & (~words_sentiments_confidence['word'].str.contains(r'\\d')) \\\n",
        "                                                                                & (words_sentiments_confidence['word'].str.len() > 1)]\n",
        "\n",
        "            words_sentiments_confidence_filtered.to_csv('/content/words_and_sentiments_filtered.csv')\n",
        "\n",
        "\n",
        "            # Finally, we look at the neutral values : Here, we use a list of buzz words that are AI related. We only want to keep\n",
        "            # the neutral words that are somewhat related to AI.\n",
        "            neutral_filter = ['ai', 'artificial', 'intelligence','machine', 'learning', 'robot']\n",
        "\n",
        "            words_sentiments_confidence_filtered_2 = words_sentiments_confidence[(words_sentiments_confidence['word'].isin(neutral_filter))]\n",
        "\n",
        "           # words_sentiments_confidence_filtered_2.to_csv('/Users/marlon/VS-Code-Projects/Youtube/words_and_sentiments_filtered_2.csv')\n",
        "\n",
        "            words_sentiments_confidence_filtered_final = pd.concat([words_sentiments_confidence_filtered, words_sentiments_confidence_filtered_2])\n",
        "\n",
        "            # Possible that we have some duplicates in the two concatenated ones (since in filtered_2 we take across also the ones with positive & negative sentiment again)\n",
        "            words_sentiments_confidence_filtered_final = words_sentiments_confidence_filtered_final.drop_duplicates()\n",
        "\n",
        "            words_sentiments_confidence_filtered_final.to_csv('/content/words_and_sentiments_filtered_final.csv')\n",
        "\n",
        "        return words_sentiments_confidence_filtered_final\n",
        "\n",
        "    if language.lower() == 'german':\n",
        "        if model.lower() == 'bert':\n",
        "            model = SentimentModel() # Specifically trained on german texts !\n",
        "\n",
        "            data = {'word' : [], 'sentiment_label' : [], 'confidence_pos' : [], 'confidence_neg' : [], 'confidence_neutral' : [], 'confidence_highest' : []}\n",
        "\n",
        "            for word in sentiment_words:\n",
        "                classes, probabilities = model.predict_sentiment([word], output_probabilities = True)\n",
        "                data['word'].append(word)\n",
        "                data['sentiment_label'].append(classes[0])\n",
        "                data['confidence_pos'].append(probabilities[0][0][1])\n",
        "                data['confidence_neg'].append(probabilities[0][1][1])\n",
        "                data['confidence_neutral'].append(probabilities[0][2][1])\n",
        "                data['confidence_highest'].append(max(probabilities[0][0][1],probabilities[0][1][1],probabilities[0][2][1]))\n",
        "\n",
        "\n",
        "            words_sentiments_confidence = pd.DataFrame(data, columns=['word', 'sentiment_label', 'confidence_pos', 'confidence_neg', 'confidence_neutral', 'confidence_highest'])\n",
        "\n",
        "            words_sentiments_confidence.to_csv('/content/words_and_sentiments.csv')\n",
        "\n",
        "            # NOTE : I keep this in the german & bert loop since I don't know if we will have models for each language that output a\n",
        "            #        a confidence score\n",
        "            # Next, based on some threshold, we only keep the words with positive / negative sentiment with a confidence >= threshold\n",
        "            # Additionally, I found this pre-trained model to give numbers a positive sentiment with high confidence, so we remove these aswell\n",
        "            # Also, sometimes it classifies a single letter with something positive/negative. Remove these aswell (in german, there are no single letter words)\n",
        "\n",
        "\n",
        "\n",
        "            words_sentiments_confidence_filtered = words_sentiments_confidence[(words_sentiments_confidence['confidence_highest'] >= threshold)\\\n",
        "                                                                                & (words_sentiments_confidence['confidence_highest'] != words_sentiments_confidence['confidence_neutral']) \\\n",
        "                                                                                & (~words_sentiments_confidence['word'].str.contains(r'\\d')) \\\n",
        "                                                                                & (words_sentiments_confidence['word'].str.len() > 1)]\n",
        "\n",
        "            words_sentiments_confidence_filtered.to_csv('/content/words_and_sentiments_filtered.csv')\n",
        "\n",
        "\n",
        "            # Finally, we look at the neutral values : Here, we use a list of buzz words that are AI related. We only want to keep\n",
        "            # the neutral words that are somewhat related to AI.\n",
        "            neutral_filter = ['ai', 'künstlich', 'künstliche', 'intelligenz', 'ki', 'machine', 'learning', 'kunst', 'roboter', 'robot']\n",
        "\n",
        "            words_sentiments_confidence_filtered_2 = words_sentiments_confidence[(words_sentiments_confidence['word'].isin(neutral_filter))]\n",
        "\n",
        "           # words_sentiments_confidence_filtered_2.to_csv('/Users/marlon/VS-Code-Projects/Youtube/words_and_sentiments_filtered_2.csv')\n",
        "\n",
        "            words_sentiments_confidence_filtered_final = pd.concat([words_sentiments_confidence_filtered, words_sentiments_confidence_filtered_2])\n",
        "\n",
        "            # Possible that we have some duplicates in the two concatenated ones (since in filtered_2 we take across also the ones with positive & negative sentiment again)\n",
        "            words_sentiments_confidence_filtered_final = words_sentiments_confidence_filtered_final.drop_duplicates()\n",
        "\n",
        "            words_sentiments_confidence_filtered_final.to_csv('/content/words_and_sentiments_filtered_final.csv')\n",
        "\n",
        "            return words_sentiments_confidence_filtered_final\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "uox2k4MNAfsB"
      },
      "outputs": [],
      "source": [
        "def P_data_remap(data_sentiments_filtered, data_lemmatized, data_only_cleaned):\n",
        "    \"\"\"\n",
        "    FILTERING APPROACH 1: After we have found the words that show some strong sentiment or are connected to AI in some way,\n",
        "    we now want to remap to the original sentences again\n",
        "    data_sentiments_filtered : the final words with all the different sentiments scores, filtered ; pd.DataFrame\n",
        "    data_lemmatized : our lemmatized (and cleaned) words ; pd.Series\n",
        "    data_only_cleaned : just cleaned data ; pd.Series\n",
        "    \"\"\"\n",
        "\n",
        "    # We first create a list of all the words\n",
        "\n",
        "    filtered_words = list(data_sentiments_filtered['word'])\n",
        "\n",
        "    # Now we only want to keep the occurences where these words appear in our lemmatized version\n",
        "\n",
        "    data_lemmatized_filtered = data_lemmatized[data_lemmatized.apply(lambda x: any(word in x for word in filtered_words))]\n",
        "\n",
        "\n",
        "    data_lemmatized_filtered.to_csv('/content/test.csv')\n",
        "\n",
        "    # And then finally we map back to the unlemmatized ones, because we will be using tokenization\n",
        "\n",
        "\n",
        "\n",
        "    data_cleaned_and_filtered = data_only_cleaned[data_only_cleaned.index.isin(data_lemmatized_filtered.index)]\n",
        "\n",
        "    data_cleaned_and_filtered.to_csv('/content/CLEANED_AND_FILTERED_APPROACH_1.csv')\n",
        "\n",
        "\n",
        "    return data_cleaned_and_filtered\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "h5domrsDAiAD"
      },
      "outputs": [],
      "source": [
        "def V_word_cloud(data):\n",
        "    \"\"\" Visualization tool. A word cloud so we can see what words appears most.\n",
        "        data : contains the counts of each word ; pd.Series\n",
        "    \"\"\"\n",
        "\n",
        "    # Convert the series to a concatenated string\n",
        "    comment_words = ' '.join([str(w) for w in data.index])\n",
        "\n",
        "    # Generate the word cloud\n",
        "    wordcloud = WordCloud(width=512, height=512, background_color='white', max_words=20).generate(comment_words)\n",
        "\n",
        "    # Display the word cloud\n",
        "    plt.figure(figsize=(10, 8), facecolor='white', edgecolor='blue')\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "7uNoumSiAkWu"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    LANGUAGE = 'english'\n",
        "    path = csv_file #'comments_0.csv'\n",
        "\n",
        "\n",
        "    data = P_data_reading(path)\n",
        "    data_cleaned = P_data_cleaning(data, language ='german') # language german here means just changing ä to ae etc. ; can also be used\n",
        "    # for english e.g.\n",
        "    data_cleaned.to_csv('/content/check_cleaned.csv')\n",
        "\n",
        "    if LANGUAGE == 'german':\n",
        "\n",
        "        data_cleaned_lemmatized = data_cleaned.apply(lambda x : P_data_lemmatizing(x, language = 'german'))\n",
        "        data_cleaned_lemmatized.to_csv('/content/check_cleaned_lemmatized.csv')\n",
        "        data_words_count, words = P_data_word_count(data_cleaned_lemmatized)\n",
        "        words_sentiments_filtered = P_data_filtering(words, model= 'bert', language= 'german')\n",
        "        data_cleaned_and_filtered = P_data_remap(words_sentiments_filtered, data_cleaned_lemmatized, data_cleaned)\n",
        "        data_cleaned_and_filtered_tokenized = data_cleaned_and_filtered.apply(lambda x : P_data_tokenization(x, language= 'german', model = 'distilbert'))\n",
        "\n",
        "        # With this final step, we can now build a dataloader and fine tune a distilBERT model\n",
        "        # since we have unlabeled data, maybe something like self-supervised learning / transfer learning (with the english youtube AI comments,\n",
        "        # like daniele proposed)\n",
        "\n",
        "\n",
        "        data_cleaned_and_filtered_tokenized.to_csv('/content/cleaned_filtered_tokenized.csv')\n",
        "\n",
        "        # Create the word cloud with the filtered data\n",
        "        data_words_count, words = P_data_word_count(data_cleaned_and_filtered)\n",
        "        V_word_cloud(data_words_count)\n",
        "\n",
        "    elif LANGUAGE == 'english':\n",
        "        data_cleaned_lemmatized = data_cleaned.apply(lambda x : P_data_lemmatizing(x, language = 'english'))\n",
        "        data_words_count, words = P_data_word_count(data_cleaned_lemmatized)\n",
        "\n",
        "        words_sentiments_filtered = P_data_filtering(words, model= 'bert', language= 'english')\n",
        "        data_cleaned_and_filtered = P_data_remap(words_sentiments_filtered, data_cleaned_lemmatized, data_cleaned)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "oS_2RbqRAo0M"
      },
      "outputs": [],
      "source": [
        "main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "vTjGADY8UtRw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13MMXj2EA7Il"
      },
      "source": [
        "# Neuer Abschnitt"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
