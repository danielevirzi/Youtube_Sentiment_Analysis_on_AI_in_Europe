{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Scraping and Filter From Youtube\n",
        "\n",
        "- Input\n",
        "    1. Youtube API Key\n",
        "    2. Playlist ID\n",
        "    3. Path for the directory\n",
        "    4. Name of the playlist scraped\n",
        "    5. Language of the videos scraped\n",
        "\n",
        "- Output\n",
        "    1. The original comments, as scraped from youtube.\n",
        "       **Ex:** *1_NEGATIVE_ENGLISH_original_comments.csv*\n",
        "\n",
        "    2. The cleaned and filtered for labeling csv, where we will then label the entries ; this file has no punctuations / stopwords.\n",
        "       **Ex:** *1_NEGATIVE_ENGLISH_cleaned_and_filtered_comments_for_labeling_LABEL_HERE.csv*\n",
        "       \n",
        "    3. The cleaned and filtered helper csv, it has the same comments as the cleaned and filtered one, but has stopwords etc. in it so it is easier to read, such that we use that to read the comments and then label them in the other file.\n",
        "       **Ex:** *1_NEGATIVE_ENGLISH_cleaned_and_filtered_comments_helper.csv*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Install and load libraries\n",
        "Remember to install the following packages before running the script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use this cell to install the required libraries on your machine\n",
        "\n",
        "# !pip install keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dH78wYL_5C7",
        "outputId": "3510f18a-f997-4715-ace2-7a06f7465405"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\giopa\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\giopa\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     C:\\Users\\giopa\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk import pos_tag\n",
        "from nltk import ne_chunk\n",
        "import regex as re\n",
        "import spacy # We need spacy for german lemmatization\n",
        "#import de_core_news_sm\n",
        "import en_core_web_sm\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "from transformers import pipeline\n",
        "from transformers import DistilBertTokenizer\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "#from germansentiment import SentimentModel\n",
        "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from googleapiclient.discovery import build\n",
        "import pandas as pd\n",
        "import getpass\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "K63x6ylOtJmR"
      },
      "outputs": [],
      "source": [
        "########### ENTER YOUR API KEY HERE ###########\n",
        "\n",
        "# You can get your API key from the Google Cloud Console \n",
        "# (https://console.cloud.google.com/apis/api/youtube.googleapis.com/credentials?hl=it&project=polished-watch-421614)\n",
        "\n",
        "api_key = 'AIzaSyCmAB3EQlCwVY9Hc8tLHr5HHaal8UzeIOQ'\n",
        "\n",
        "########### ENTER THE PLAYLIST ID HERE ###########\n",
        "\n",
        "# Make a playlist on YouTube and copy the playlist ID from the URL\n",
        "\n",
        "playlist_ids = ['PLi6PHirMO4RB17Ja_L2XZhVxdu_1OzD8m']\n",
        "\n",
        "########### ENTER THE PATH TO THE FOLDER WHERE YOU WANT TO SAVE THE CSV FILE ###########\n",
        "\n",
        "# Change the last number to the number of the playlist you are scraping\n",
        "# Look at the Google Sheet to see which number corresponds to which playlist based on the color\n",
        "\n",
        "path = r'C:\\Users\\giopa\\Downloads\\1' # Example for windows: r'C:\\Users\\danie\\Downloads\\1', r'C:\\Users\\danie\\Downloads\\2', r'C:\\Users\\danie\\Downloads\\3\n",
        "                                     # Example for mac: '/Users/danie/Downloads/1', '/Users/danie/Downloads/2', '/Users/danie/Downloads/3'\n",
        "\n",
        "########### ENTER THE NAME OF THE PLAYLIST YOU ARE SCRAPING HERE ###########\n",
        "\n",
        "# This is the name of the playlist you are scraping. It will be used to name the CSV file\n",
        "\n",
        "csv_file = \"_NEUTRAL_ENGLISH\" # Example: \"_POSITIVE_ENGLISH\", \"_NEUTRAL_ENGLISH\", \"_POSITIVE_GERMAN\", \"_NEGATIVE_GERMAN\"\n",
        "\n",
        "########### ENTER THE LANGUAGE FOR SCRAPING ###########\n",
        "\n",
        "language = 'english'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Scraping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "DDYx6qk2tP5Y"
      },
      "outputs": [],
      "source": [
        "# Build the YouTube client\n",
        "youtube = build('youtube', 'v3', developerKey=api_key)\n",
        "\n",
        "def get_all_video_ids_from_playlists(youtube, playlist_ids):\n",
        "    all_videos = []  # Initialize a single list to hold all video IDs\n",
        "\n",
        "    for playlist_id in playlist_ids:\n",
        "        next_page_token = None\n",
        "\n",
        "        # Fetch videos from the current playlist\n",
        "        while True:\n",
        "            playlist_request = youtube.playlistItems().list(\n",
        "                part='contentDetails',\n",
        "                playlistId=playlist_id,\n",
        "                maxResults=50,\n",
        "                pageToken=next_page_token)\n",
        "            playlist_response = playlist_request.execute()\n",
        "\n",
        "            all_videos += [item['contentDetails']['videoId'] for item in playlist_response['items']]\n",
        "\n",
        "            next_page_token = playlist_response.get('nextPageToken')\n",
        "\n",
        "            if next_page_token is None:\n",
        "                break\n",
        "\n",
        "    return all_videos\n",
        "\n",
        "# Fetch all video IDs from the specified playlists\n",
        "video_ids = get_all_video_ids_from_playlists(youtube, playlist_ids)\n",
        "\n",
        "\n",
        "\n",
        "# Function to get replies for a specific comment\n",
        "def get_replies(youtube, parent_id, video_id):  # Added video_id as an argument\n",
        "    replies = []\n",
        "    next_page_token = None\n",
        "\n",
        "    while True:\n",
        "        reply_request = youtube.comments().list(\n",
        "            part=\"snippet\",\n",
        "            parentId=parent_id,\n",
        "            textFormat=\"plainText\",\n",
        "            maxResults=100,\n",
        "            pageToken=next_page_token\n",
        "        )\n",
        "        reply_response = reply_request.execute()\n",
        "\n",
        "        for item in reply_response['items']:\n",
        "            comment = item['snippet']\n",
        "            replies.append({\n",
        "                'Timestamp': comment['publishedAt'],\n",
        "                'Username': comment['authorDisplayName'],\n",
        "                'VideoID': video_id,\n",
        "                'Comment': comment['textDisplay'],\n",
        "                'Date': comment['updatedAt'] if 'updatedAt' in comment else comment['publishedAt']\n",
        "            })\n",
        "\n",
        "        next_page_token = reply_response.get('nextPageToken')\n",
        "        if not next_page_token:\n",
        "            break\n",
        "\n",
        "    return replies\n",
        "\n",
        "# Function to get all comments (including replies) for a single video\n",
        "def get_comments_for_video(youtube, video_id):\n",
        "    all_comments = []\n",
        "    next_page_token = None\n",
        "\n",
        "    while True:\n",
        "        comment_request = youtube.commentThreads().list(\n",
        "            part=\"snippet\",\n",
        "            videoId=video_id,\n",
        "            pageToken=next_page_token,\n",
        "            textFormat=\"plainText\",\n",
        "            maxResults=100\n",
        "        )\n",
        "        comment_response = comment_request.execute()\n",
        "\n",
        "        for item in comment_response['items']:\n",
        "            top_comment = item['snippet']['topLevelComment']['snippet']\n",
        "            all_comments.append({\n",
        "                'Timestamp': top_comment['publishedAt'],\n",
        "                'Username': top_comment['authorDisplayName'],\n",
        "                'VideoID': video_id,  # Directly using video_id from function parameter\n",
        "                'Comment': top_comment['textDisplay'],\n",
        "                'Date': top_comment['updatedAt'] if 'updatedAt' in top_comment else top_comment['publishedAt']\n",
        "            })\n",
        "\n",
        "            # Fetch replies if there are any\n",
        "            if item['snippet']['totalReplyCount'] > 0:\n",
        "                all_comments.extend(get_replies(youtube, item['snippet']['topLevelComment']['id'], video_id))\n",
        "\n",
        "        next_page_token = comment_response.get('nextPageToken')\n",
        "        if not next_page_token:\n",
        "            break\n",
        "\n",
        "    return all_comments\n",
        "\n",
        "# List to hold all comments from all videos\n",
        "all_comments = []\n",
        "\n",
        "\n",
        "for video_id in video_ids:\n",
        "    video_comments = get_comments_for_video(youtube, video_id)\n",
        "    all_comments.extend(video_comments)\n",
        "\n",
        "# Create DataFrame\n",
        "comments_df = pd.DataFrame(all_comments)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "z_J8QsqMthUQ"
      },
      "outputs": [],
      "source": [
        "########### ENTER EXPORT LOCATION HERE ###########\n",
        "\n",
        "# Export whole dataset to the local machine as CSV File\n",
        "comments_df.to_csv(path + csv_file + '_original_comments.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ct5p778A_xQY"
      },
      "outputs": [],
      "source": [
        "def remove_emoji(comment):\n",
        "    \"\"\"Function to remove emojis.\n",
        "        comment : data input ; str\n",
        "        Taken from :\n",
        "        https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
        "                               u\"\\U00002702-\\U000027B0\"\n",
        "                               u\"\\U00002702-\\U000027B0\"\n",
        "                               u\"\\U000024C2-\\U0001F251\"\n",
        "                               u\"\\U0001f926-\\U0001f937\"\n",
        "                               u\"\\U00010000-\\U0010ffff\"\n",
        "                               u\"\\u2640-\\u2642\"\n",
        "                               u\"\\u2600-\\u2B55\"\n",
        "                               u\"\\u200d\"\n",
        "                               u\"\\u23cf\"\n",
        "                               u\"\\u23e9\"\n",
        "                               u\"\\u231a\"\n",
        "                               u\"\\ufe0f\"  # dingbats\n",
        "                               u\"\\u3030\"\n",
        "                               \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', comment)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "sP0E8avz_2SA"
      },
      "outputs": [],
      "source": [
        "def P_data_reading(path_to_data):\n",
        "    \"\"\"Simple function to read in the data we want to use.\n",
        "       path_to_data : the path pointing to our data ; csv file\n",
        "    \"\"\"\n",
        "\n",
        "    comments_data = pd.read_csv(path_to_data)\n",
        "\n",
        "    ############### FOR TESTING PURPOSES, WE ONLY TAKE FIRST 20 ###############\n",
        "\n",
        "    # Turn into Series, containing only the comments\n",
        "    return comments_data['Comment']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ikhPOFTZ_7l4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<>:19: SyntaxWarning: invalid escape sequence '\\w'\n",
            "<>:19: SyntaxWarning: invalid escape sequence '\\w'\n",
            "C:\\Users\\giopa\\AppData\\Local\\Temp\\ipykernel_8684\\3634153772.py:19: SyntaxWarning: invalid escape sequence '\\w'\n",
            "  data = data.str.replace('@@\\w+', '', regex=True)\n"
          ]
        }
      ],
      "source": [
        "def P_data_cleaning(data, language, labelling):\n",
        "    \"\"\"Function to clean our data.\n",
        "       data : data input ; pd.Series\n",
        "       language : what language the comments are in (input in lowercase) : str\n",
        "       labelling : if we want to label, we keep punctuation & stopwords\n",
        "    \"\"\"\n",
        "\n",
        "    # REMOVE NAN ENTRIES\n",
        "    data = data.dropna()\n",
        "\n",
        "    # REMOVE COMMENTS THAT EXCEED CERTAIN LENGTH (350 for now)\n",
        "    data = data[data.str.len() <= 350]\n",
        "    \n",
        "\n",
        "    # FOR GERMAN DATA : Change ö , ä , ü to oe, ae, ue\n",
        "    data = data.str.replace(\"ö\", \"oe\").str.replace(\"ä\", \"ae\").str.replace(\"ü\", \"ue\")\n",
        "\n",
        "    # REMOVE NAMES FROM ANSWERS (in youtube comments scraper answers stored by @@)\n",
        "    data = data.str.replace('@@\\w+', '', regex=True)\n",
        "\n",
        "    # REMOVING PUNCTUATION\n",
        "    if labelling == False:\n",
        "      data = data.str.replace('[^a-zA-Z0-9]',' ')\n",
        "\n",
        "    # REMOVING EMOJIS\n",
        "    data = data.apply(lambda x: remove_emoji(x))\n",
        "\n",
        "    # LOWERCASE\n",
        "    data = data.str.lower()\n",
        "\n",
        "    # REMOVING STOPWORDS\n",
        "    if labelling == False:\n",
        "      data = data.apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords.words(language))]))\n",
        "\n",
        "\n",
        "    return data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "EZ6GKG7IATdR"
      },
      "outputs": [],
      "source": [
        "def P_data_tokenization(comment, language, model):\n",
        "    \"\"\"\n",
        "    Tokenization function. We implement different tokenizers\n",
        "    comment : the current comment to analyze ; string\n",
        "    language : the language for tokenization ; string\n",
        "    model : the tokenizer we are using (or from which model we are using the tokenizer from)\n",
        "    \"\"\"\n",
        "\n",
        "    if model.lower() == 'distilbert':\n",
        "        # We use the distilBERT tokenization (in case we are going to use that model later on)\n",
        "        # NOTE : don't know what languages are included in multilingual, I just know german is in it\n",
        "        tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-multilingual-cased\")\n",
        "\n",
        "      #  encoded_comment = tokenizer.encode(comment, add_special_tokens=True)\n",
        "\n",
        "\n",
        "        # Tokenizing and padding comments (padding needed for constant input later on in distilBERT model)\n",
        "        tokenized_comment = tokenizer.encode_plus(\n",
        "            comment,\n",
        "            max_length=128,  # Set the desired maximum sequence length\n",
        "            padding='longest',  # Pad to the longest sequence in the batch\n",
        "            truncation=True,  # Truncate if needed\n",
        "            return_tensors='pt',  # Return PyTorch tensors\n",
        "            )\n",
        "\n",
        "        # Access the input IDs (we'll use these for fine-tuning (? on which data will we do fine-tuning ? Daniele proposed\n",
        "        # the english comment section for AI on youtube, because there we have so much data and it is similar to ours))\n",
        "        #input_ids = tokenized_comment['input_ids']\n",
        "\n",
        "        # Will return input ids and attention mask of our inputs\n",
        "        return tokenized_comment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "mkojlm8sAY8_"
      },
      "outputs": [],
      "source": [
        "def P_data_lemmatizing(comment, language):\n",
        "    \"\"\"FILTER APPROACH 1 : We first lemmatize so we get the base words of everything\n",
        "       + we have less words in general and can build bigger groups\n",
        "       - we will lose some accuracy in our sentiment analysis : words like best/better/good will all be just good\n",
        "\n",
        "       To combat the negative effect, we will do the following : Build a mapping between the original input sentences\n",
        "       and the lemmatized ones. We will just lemmatize to build the bigger groups and denoise our dataset. Then, when we\n",
        "       done this, we map back to the original sentences and tokenize.\n",
        "\n",
        "       Since we use pandas, we just won't reset indices. That way, we just keep the original pandas dataset (i.e. we save\n",
        "       a copy of it after the cleaning steps and right before lemmatizing) and then use the indices for our mapping.\n",
        "\n",
        "       comment : the current comment to analyze ; string\n",
        "       language : the language for tokenization ; string\n",
        "    \"\"\"\n",
        "    if language.lower() == 'german':\n",
        "        lemmatizer = spacy.load(\"de_core_news_sm\")\n",
        "\n",
        "\n",
        "    if language.lower() == 'english':\n",
        "        lemmatizer = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "\n",
        "\n",
        "    lemmatized_comment = ' '.join([token.lemma_ for token in lemmatizer(comment)])\n",
        "\n",
        "    # After lemmatizing, some words are again higher cased\n",
        "    lemmatized_comment = lemmatized_comment.lower()\n",
        "\n",
        "\n",
        "    return lemmatized_comment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "qmKV6_1TAbx8"
      },
      "outputs": [],
      "source": [
        "def P_data_word_count(data):\n",
        "    \"\"\"FILTER APPROACH 1 : We find the buzz words we want to filter for.\n",
        "       The idea is to iterate through our own data and see if there are\n",
        "       some really common words that are used for showing ones sentiment\n",
        "       if there is a pattern, we can use these to remove the noise from\n",
        "       our data\n",
        "       data : data input : pd.Series\n",
        "    \"\"\"\n",
        "    # explode() : convert each single element into a row\n",
        "    # We also sort them to find the most common ones\n",
        "    word_counts = data.str.split().explode().value_counts().sort_values(ascending = False)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # We return the count aswell as the (lemmatized) words themselves\n",
        "    return word_counts, list(word_counts.index)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "cgHekKQIAe0x"
      },
      "outputs": [],
      "source": [
        "def P_data_filtering(sentiment_words, model, language, threshold = 0.95):\n",
        "    \"\"\"FILTERING APPROACH 1 : We do pre-filtering on our data to remove noise.\n",
        "       For this, we use pre-trained, state-of-the-art models to find the sentiments of different words in different languages.\n",
        "       Next, we filter the data (see details below)\n",
        "       sentiment_words = list of words we want to use for filtering : List of String\n",
        "       model : which model to use\n",
        "       language : the language for tokenization ; string\n",
        "       threshold : threshold on the confidence level of sentiment predictions of the single words ; Float\n",
        "    \"\"\"\n",
        "\n",
        "    if language.lower() == 'english':\n",
        "        # According to :\n",
        "        # https://huggingface.co/rabindralamsal/BERTsent?text=I+like+you.+I+love+you\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\"rabindralamsal/BERTsent\")\n",
        "        model = TFAutoModelForSequenceClassification.from_pretrained(\"rabindralamsal/BERTsent\")\n",
        "\n",
        "        data = {'word' : [], 'sentiment_label' : [], 'confidence_pos' : [], 'confidence_neg' : [], 'confidence_neutral' : [], 'confidence_highest' : []}\n",
        "\n",
        "        for word in sentiment_words:\n",
        "\n",
        "            input = tokenizer.encode(word, return_tensors=\"tf\")\n",
        "            output = model.predict(input)[0]\n",
        "            prediction = tf.nn.softmax(output, axis=1).numpy()\n",
        "            sentiment = np.argmax(prediction)\n",
        "\n",
        "            # Convert sentiments (as they are stored 0,1,2 in this model)\n",
        "            if sentiment == 0:\n",
        "                sentiment = 'negative'\n",
        "            elif sentiment == 1:\n",
        "                sentiment = 'neutral'\n",
        "            elif sentiment == 2:\n",
        "                sentiment = 'positive'\n",
        "\n",
        "\n",
        "            data['word'].append(word)\n",
        "            data['sentiment_label'].append(sentiment)\n",
        "            data['confidence_pos'].append(prediction[0][2])\n",
        "            data['confidence_neg'].append(prediction[0][0])\n",
        "            data['confidence_neutral'].append(prediction[0][1])\n",
        "            data['confidence_highest'].append(max(prediction[0][0],prediction[0][1],prediction[0][2]))\n",
        "\n",
        "\n",
        "            words_sentiments_confidence = pd.DataFrame(data, columns=['word', 'sentiment_label', 'confidence_pos', 'confidence_neg', 'confidence_neutral', 'confidence_highest'])\n",
        "        \n",
        "\n",
        "\n",
        "            words_sentiments_confidence_filtered = words_sentiments_confidence[(words_sentiments_confidence['confidence_highest'] >= threshold)\\\n",
        "                                                                                & (words_sentiments_confidence['confidence_highest'] != words_sentiments_confidence['confidence_neutral']) \\\n",
        "                                                                                & (~words_sentiments_confidence['word'].str.contains(r'\\d')) \\\n",
        "                                                                                & (words_sentiments_confidence['word'].str.len() > 1)]\n",
        "\n",
        "       \n",
        "\n",
        "\n",
        "            # Finally, we look at the neutral values : Here, we use a list of buzz words that are AI related. We only want to keep\n",
        "            # the neutral words that are somewhat related to AI.\n",
        "            neutral_filter = ['ai', 'artificial', 'intelligence','machine', 'learning', 'robot']\n",
        "\n",
        "            words_sentiments_confidence_filtered_2 = words_sentiments_confidence[(words_sentiments_confidence['word'].isin(neutral_filter))]\n",
        "\n",
        "      \n",
        "\n",
        "            words_sentiments_confidence_filtered_final = pd.concat([words_sentiments_confidence_filtered, words_sentiments_confidence_filtered_2])\n",
        "\n",
        "            # Possible that we have some duplicates in the two concatenated ones (since in filtered_2 we take across also the ones with positive & negative sentiment again)\n",
        "            words_sentiments_confidence_filtered_final = words_sentiments_confidence_filtered_final.drop_duplicates()\n",
        "\n",
        "\n",
        "        return words_sentiments_confidence_filtered_final\n",
        "\n",
        "    if language.lower() == 'german':\n",
        "        if model.lower() == 'bert':\n",
        "            model = SentimentModel() # Specifically trained on german texts !\n",
        "\n",
        "            data = {'word' : [], 'sentiment_label' : [], 'confidence_pos' : [], 'confidence_neg' : [], 'confidence_neutral' : [], 'confidence_highest' : []}\n",
        "\n",
        "            for word in sentiment_words:\n",
        "                classes, probabilities = model.predict_sentiment([word], output_probabilities = True)\n",
        "                data['word'].append(word)\n",
        "                data['sentiment_label'].append(classes[0])\n",
        "                data['confidence_pos'].append(probabilities[0][0][1])\n",
        "                data['confidence_neg'].append(probabilities[0][1][1])\n",
        "                data['confidence_neutral'].append(probabilities[0][2][1])\n",
        "                data['confidence_highest'].append(max(probabilities[0][0][1],probabilities[0][1][1],probabilities[0][2][1]))\n",
        "\n",
        "\n",
        "            words_sentiments_confidence = pd.DataFrame(data, columns=['word', 'sentiment_label', 'confidence_pos', 'confidence_neg', 'confidence_neutral', 'confidence_highest'])\n",
        "\n",
        "            # NOTE : I keep this in the german & bert loop since I don't know if we will have models for each language that output a\n",
        "            #        a confidence score\n",
        "            # Next, based on some threshold, we only keep the words with positive / negative sentiment with a confidence >= threshold\n",
        "            # Additionally, I found this pre-trained model to give numbers a positive sentiment with high confidence, so we remove these aswell\n",
        "            # Also, sometimes it classifies a single letter with something positive/negative. Remove these aswell (in german, there are no single letter words)\n",
        "\n",
        "\n",
        "\n",
        "            words_sentiments_confidence_filtered = words_sentiments_confidence[(words_sentiments_confidence['confidence_highest'] >= threshold)\\\n",
        "                                                                                & (words_sentiments_confidence['confidence_highest'] != words_sentiments_confidence['confidence_neutral']) \\\n",
        "                                                                                & (~words_sentiments_confidence['word'].str.contains(r'\\d')) \\\n",
        "                                                                                & (words_sentiments_confidence['word'].str.len() > 1)]\n",
        "\n",
        "    \n",
        "            # Finally, we look at the neutral values : Here, we use a list of buzz words that are AI related. We only want to keep\n",
        "            # the neutral words that are somewhat related to AI.\n",
        "            neutral_filter = ['ai', 'künstlich', 'künstliche', 'intelligenz', 'ki', 'machine', 'learning', 'kunst', 'roboter', 'robot']\n",
        "            words_sentiments_confidence_filtered_2 = words_sentiments_confidence[(words_sentiments_confidence['word'].isin(neutral_filter))]\n",
        "            words_sentiments_confidence_filtered_final = pd.concat([words_sentiments_confidence_filtered, words_sentiments_confidence_filtered_2])\n",
        "            # Possible that we have some duplicates in the two concatenated ones (since in filtered_2 we take across also the ones with positive & negative sentiment again)\n",
        "            words_sentiments_confidence_filtered_final = words_sentiments_confidence_filtered_final.drop_duplicates()\n",
        "\n",
        "            return words_sentiments_confidence_filtered_final\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "uox2k4MNAfsB"
      },
      "outputs": [],
      "source": [
        "def P_data_remap(data_sentiments_filtered, data_lemmatized, data_only_cleaned_for_labeling, data_only_cleaned, path, csv_file):\n",
        "    \"\"\"\n",
        "    FILTERING APPROACH 1: After we have found the words that show some strong sentiment or are connected to AI in some way,\n",
        "    we now want to remap to the original sentences again\n",
        "    data_sentiments_filtered : the final words with all the different sentiments scores, filtered ; pd.DataFrame\n",
        "    data_lemmatized : our lemmatized (and cleaned) words ; pd.Series\n",
        "    data_only_cleaned : just cleaned data ; pd.Series\n",
        "    \"\"\"\n",
        "\n",
        "    # We first create a list of all the words\n",
        "\n",
        "    filtered_words = list(data_sentiments_filtered['word'])\n",
        "\n",
        "    # Now we only want to keep the occurences where these words appear in our lemmatized version\n",
        "\n",
        "    data_lemmatized_filtered = data_lemmatized[data_lemmatized.apply(lambda x: any(word in x for word in filtered_words))]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # And then finally we map back to the unlemmatized ones, because we will be using tokenization\n",
        "\n",
        "\n",
        "    data_cleaned_and_filtered = data_only_cleaned[data_only_cleaned.index.isin(data_lemmatized_filtered.index)]\n",
        "    data_cleaned_and_filtered_for_labeling = data_only_cleaned_for_labeling[data_only_cleaned_for_labeling.index.isin(data_lemmatized_filtered.index)]\n",
        "\n",
        "    data_cleaned_and_filtered_for_labeling.to_csv(path + csv_file + '_cleaned_and_filtered_comments_helper.csv')\n",
        "    data_cleaned_and_filtered.to_csv(path + csv_file + '_cleaned_and_filtered_comments_for_labeling_LABEL_HERE.csv')\n",
        "\n",
        "\n",
        "    return  data_cleaned_and_filtered_for_labeling, data_cleaned_and_filtered\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "h5domrsDAiAD"
      },
      "outputs": [],
      "source": [
        "def V_word_cloud(data):\n",
        "    \"\"\" Visualization tool. A word cloud so we can see what words appears most.\n",
        "        data : contains the counts of each word ; pd.Series\n",
        "    \"\"\"\n",
        "\n",
        "    # Convert the series to a concatenated string\n",
        "    comment_words = ' '.join([str(w) for w in data.index])\n",
        "\n",
        "    # Generate the word cloud\n",
        "    wordcloud = WordCloud(width=512, height=512, background_color='white', max_words=20).generate(comment_words)\n",
        "\n",
        "    # Display the word cloud\n",
        "    plt.figure(figsize=(10, 8), facecolor='white', edgecolor='blue')\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Filter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "7uNoumSiAkWu"
      },
      "outputs": [],
      "source": [
        "# Main function to run the whole pipeline\n",
        "\n",
        "def main():\n",
        "\n",
        "    # Set the language\n",
        "    LANGUAGE = language\n",
        "\n",
        "    # Read in the data\n",
        "    data = P_data_reading(path + csv_file + '_original_comments.csv')\n",
        "\n",
        "    # Clean the data\n",
        "    data_cleaned = P_data_cleaning(data, language = language , labelling = False) # language german here means just changing ä to ae etc. ; can be used for english aswell\n",
        "    data_cleaned_for_labeling = P_data_cleaning(data, language = language, labelling = True) # We need this to map back to originals later on\n",
        " \n",
        "\n",
        "\n",
        "\n",
        "    if LANGUAGE == 'german':\n",
        "        \n",
        "        # We first lemmatize the data\n",
        "        data_cleaned_lemmatized = data_cleaned.apply(lambda x : P_data_lemmatizing(x,\n",
        "                                                                                   language = LANGUAGE))\n",
        "        \n",
        "        # We then count the words\n",
        "        data_words_count, words = P_data_word_count(data_cleaned_lemmatized)\n",
        "\n",
        "        # We then filter the words\n",
        "        words_sentiments_filtered = P_data_filtering(words, \n",
        "                                                     model= 'bert', \n",
        "                                                     language= LANGUAGE)\n",
        "\n",
        "        # We then remap the data\n",
        "        data_cleaned_and_filtered_for_labeling, data_cleaned_and_filtered = P_data_remap(words_sentiments_filtered, \n",
        "                                                                                         data_cleaned_lemmatized, \n",
        "                                                                                         data_cleaned_for_labeling, \n",
        "                                                                                         data_cleaned, \n",
        "                                                                                         path, \n",
        "                                                                                         csv_file)\n",
        "\n",
        "\n",
        "\n",
        "    elif LANGUAGE == 'english':\n",
        "        \n",
        "        # We first lemmatize the data\n",
        "        data_cleaned_lemmatized = data_cleaned.apply(lambda x : P_data_lemmatizing(x, \n",
        "                                                                                   language = LANGUAGE)) \n",
        "\n",
        "        # We then count the words\n",
        "        data_words_count, words = P_data_word_count(data_cleaned_lemmatized)\n",
        "\n",
        "        # We then filter the words\n",
        "        words_sentiments_filtered = P_data_filtering(words, \n",
        "                                                     model= 'bert', \n",
        "                                                     language= LANGUAGE)\n",
        "        \n",
        "        # We then remap the data\n",
        "        data_cleaned_and_filtered_for_labeling, data_cleaned_and_filtered = P_data_remap(words_sentiments_filtered, \n",
        "                                                                                         data_cleaned_lemmatized, \n",
        "                                                                                         data_cleaned_for_labeling, \n",
        "                                                                                         data_cleaned, \n",
        "                                                                                         path, \n",
        "                                                                                         csv_file)\n",
        "    \n",
        "\n",
        "    return data_cleaned_and_filtered_for_labeling, data_cleaned_and_filtered\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "oS_2RbqRAo0M"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\giopa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aa236b9ce0294fbbbd6d4530c63e9335",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/323 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\giopa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\giopa\\.cache\\huggingface\\hub\\models--rabindralamsal--BERTsent. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "38eb3f175d1d4262a92149a220dd5c8a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/843k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "104e198b3a4f4467977a81409bdddb0f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "bpe.codes:   0%|          | 0.00/1.08M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "67fea47295ab47749b4a947d27217eb2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "added_tokens.json:   0%|          | 0.00/17.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "658aacee143c48a49910a7342077bc84",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/150 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "emoji is not installed, thus not converting emoticons or emojis into text. Install emoji: pip3 install emoji==0.6.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8dbcd4eee9b4471f97a15c881e4b3c0b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/929 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "RuntimeError",
          "evalue": "Failed to import transformers.models.roberta.modeling_tf_roberta because of the following error (look up to see its traceback):\nYour currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "File \u001b[1;32mc:\\Users\\giopa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\activations_tf.py:22\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 22\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mImportError\u001b[39;00m):\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tf_keras'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "File \u001b[1;32mc:\\Users\\giopa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1510\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1510\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "File \u001b[1;32mc:\\Users\\giopa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\importlib\\__init__.py:90\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m     89\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap>:1387\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap>:1331\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap>:935\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap_external>:994\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap>:488\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
            "File \u001b[1;32mc:\\Users\\giopa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_tf_roberta.py:28\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations_tf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_tf_activation\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_tf_outputs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     30\u001b[0m     TFBaseModelOutputWithPastAndCrossAttentions,\n\u001b[0;32m     31\u001b[0m     TFBaseModelOutputWithPoolingAndCrossAttentions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     37\u001b[0m     TFTokenClassifierOutput,\n\u001b[0;32m     38\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\giopa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\activations_tf.py:27\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m parse(keras\u001b[38;5;241m.\u001b[39m__version__)\u001b[38;5;241m.\u001b[39mmajor \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m---> 27\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     28\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour currently installed version of Keras is Keras 3, but this is not yet supported in \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     29\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTransformers. Please install the backwards-compatible tf-keras package with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     30\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`pip install tf-keras`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     31\u001b[0m         )\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_gelu\u001b[39m(x):\n",
            "\u001b[1;31mValueError\u001b[0m: Your currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`.",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[15], line 52\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     49\u001b[0m data_words_count, words \u001b[38;5;241m=\u001b[39m P_data_word_count(data_cleaned_lemmatized)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# We then filter the words\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m words_sentiments_filtered \u001b[38;5;241m=\u001b[39m \u001b[43mP_data_filtering\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m                                             \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbert\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                             \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mLANGUAGE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# We then remap the data\u001b[39;00m\n\u001b[0;32m     57\u001b[0m data_cleaned_and_filtered_for_labeling, data_cleaned_and_filtered \u001b[38;5;241m=\u001b[39m P_data_remap(words_sentiments_filtered, \n\u001b[0;32m     58\u001b[0m                                                                                  data_cleaned_lemmatized, \n\u001b[0;32m     59\u001b[0m                                                                                  data_cleaned_for_labeling, \n\u001b[0;32m     60\u001b[0m                                                                                  data_cleaned, \n\u001b[0;32m     61\u001b[0m                                                                                  path, \n\u001b[0;32m     62\u001b[0m                                                                                  csv_file)\n",
            "Cell \u001b[1;32mIn[12], line 15\u001b[0m, in \u001b[0;36mP_data_filtering\u001b[1;34m(sentiment_words, model, language, threshold)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m language\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;66;03m# According to :\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;66;03m# https://huggingface.co/rabindralamsal/BERTsent?text=I+like+you.+I+love+you\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrabindralamsal/BERTsent\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 15\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mTFAutoModelForSequenceClassification\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrabindralamsal/BERTsent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m     data \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mword\u001b[39m\u001b[38;5;124m'\u001b[39m : [], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiment_label\u001b[39m\u001b[38;5;124m'\u001b[39m : [], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfidence_pos\u001b[39m\u001b[38;5;124m'\u001b[39m : [], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfidence_neg\u001b[39m\u001b[38;5;124m'\u001b[39m : [], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfidence_neutral\u001b[39m\u001b[38;5;124m'\u001b[39m : [], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfidence_highest\u001b[39m\u001b[38;5;124m'\u001b[39m : []}\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m sentiment_words:\n",
            "File \u001b[1;32mc:\\Users\\giopa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:562\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    558\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    559\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    560\u001b[0m     )\n\u001b[0;32m    561\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m--> 562\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m \u001b[43m_get_model_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model_mapping\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    564\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    565\u001b[0m     )\n\u001b[0;32m    566\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    567\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    569\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\giopa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:383\u001b[0m, in \u001b[0;36m_get_model_class\u001b[1;34m(config, model_mapping)\u001b[0m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_model_class\u001b[39m(config, model_mapping):\n\u001b[1;32m--> 383\u001b[0m     supported_models \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_mapping\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    384\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(supported_models, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m    385\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m supported_models\n",
            "File \u001b[1;32mc:\\Users\\giopa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:734\u001b[0m, in \u001b[0;36m_LazyAutoMapping.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    732\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_type \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping:\n\u001b[0;32m    733\u001b[0m     model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping[model_type]\n\u001b[1;32m--> 734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_attr_from_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    736\u001b[0m \u001b[38;5;66;03m# Maybe there was several model types associated with this config.\u001b[39;00m\n\u001b[0;32m    737\u001b[0m model_types \u001b[38;5;241m=\u001b[39m [k \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config_mapping\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;241m==\u001b[39m key\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m]\n",
            "File \u001b[1;32mc:\\Users\\giopa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:748\u001b[0m, in \u001b[0;36m_LazyAutoMapping._load_attr_from_module\u001b[1;34m(self, model_type, attr)\u001b[0m\n\u001b[0;32m    746\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m module_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules:\n\u001b[0;32m    747\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules[module_name] \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransformers.models\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 748\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgetattribute_from_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_modules\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattr\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\giopa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:692\u001b[0m, in \u001b[0;36mgetattribute_from_module\u001b[1;34m(module, attr)\u001b[0m\n\u001b[0;32m    690\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(attr, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    691\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(getattribute_from_module(module, a) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m attr)\n\u001b[1;32m--> 692\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mhasattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattr\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    693\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(module, attr)\n\u001b[0;32m    694\u001b[0m \u001b[38;5;66;03m# Some of the mappings have entries model_type -> object of another model type. In that case we try to grab the\u001b[39;00m\n\u001b[0;32m    695\u001b[0m \u001b[38;5;66;03m# object at the top level.\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\giopa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1500\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1498\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(name)\n\u001b[0;32m   1499\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m-> 1500\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1501\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\giopa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1512\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1510\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 1512\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1513\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1514\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1515\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
            "\u001b[1;31mRuntimeError\u001b[0m: Failed to import transformers.models.roberta.modeling_tf_roberta because of the following error (look up to see its traceback):\nYour currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`."
          ]
        }
      ],
      "source": [
        "main()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
