{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Loading egg at /opt/homebrew/lib/python3.11/site-packages/jupyter-1.0.0-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: pip in /opt/homebrew/lib/python3.11/site-packages (24.1)\n",
      "Found existing installation: transformers 4.41.2\n",
      "Uninstalling transformers-4.41.2:\n",
      "  Successfully uninstalled transformers-4.41.2\n",
      "\u001b[33mDEPRECATION: Loading egg at /opt/homebrew/lib/python3.11/site-packages/jupyter-1.0.0-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting transformers\n",
      "  Using cached transformers-4.41.2-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/lib/python3.11/site-packages (from transformers) (3.12.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /opt/homebrew/lib/python3.11/site-packages (from transformers) (0.23.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/homebrew/lib/python3.11/site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/lib/python3.11/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/homebrew/lib/python3.11/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/homebrew/lib/python3.11/site-packages (from transformers) (2023.8.8)\n",
      "Requirement already satisfied: requests in /opt/homebrew/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/homebrew/lib/python3.11/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/homebrew/lib/python3.11/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/homebrew/lib/python3.11/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/homebrew/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/homebrew/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/lib/python3.11/site-packages (from requests->transformers) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.11/site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.11/site-packages (from requests->transformers) (2023.5.7)\n",
      "Using cached transformers-4.41.2-py3-none-any.whl (9.1 MB)\n",
      "Installing collected packages: transformers\n",
      "Successfully installed transformers-4.41.2\n",
      "\u001b[33mDEPRECATION: Loading egg at /opt/homebrew/lib/python3.11/site-packages/jupyter-1.0.0-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: transformers in /opt/homebrew/lib/python3.11/site-packages (4.41.2)\n",
      "Requirement already satisfied: sentencepiece in /opt/homebrew/lib/python3.11/site-packages (0.2.0)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/lib/python3.11/site-packages (from transformers) (3.12.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /opt/homebrew/lib/python3.11/site-packages (from transformers) (0.23.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/homebrew/lib/python3.11/site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/lib/python3.11/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/homebrew/lib/python3.11/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/homebrew/lib/python3.11/site-packages (from transformers) (2023.8.8)\n",
      "Requirement already satisfied: requests in /opt/homebrew/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/homebrew/lib/python3.11/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/homebrew/lib/python3.11/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/homebrew/lib/python3.11/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/homebrew/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/homebrew/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/lib/python3.11/site-packages (from requests->transformers) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.11/site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.11/site-packages (from requests->transformers) (2023.5.7)\n",
      "\u001b[33mDEPRECATION: Loading egg at /opt/homebrew/lib/python3.11/site-packages/jupyter-1.0.0-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: datasets in /opt/homebrew/lib/python3.11/site-packages (2.19.1)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/lib/python3.11/site-packages (from datasets) (3.12.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/homebrew/lib/python3.11/site-packages (from datasets) (1.23.5)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /opt/homebrew/lib/python3.11/site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/homebrew/lib/python3.11/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/homebrew/lib/python3.11/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/homebrew/lib/python3.11/site-packages (from datasets) (1.5.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/homebrew/lib/python3.11/site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/homebrew/lib/python3.11/site-packages (from datasets) (4.65.0)\n",
      "Requirement already satisfied: xxhash in /opt/homebrew/lib/python3.11/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/homebrew/lib/python3.11/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in /opt/homebrew/lib/python3.11/site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in /opt/homebrew/lib/python3.11/site-packages (from datasets) (3.8.4)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /opt/homebrew/lib/python3.11/site-packages (from datasets) (0.23.4)\n",
      "Requirement already satisfied: packaging in /opt/homebrew/lib/python3.11/site-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/homebrew/lib/python3.11/site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp->datasets) (3.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/homebrew/lib/python3.11/site-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (2023.5.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/homebrew/lib/python3.11/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/lib/python3.11/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/lib/python3.11/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "\u001b[33mDEPRECATION: Loading egg at /opt/homebrew/lib/python3.11/site-packages/jupyter-1.0.0-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: accelerate in /opt/homebrew/lib/python3.11/site-packages (0.31.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/homebrew/lib/python3.11/site-packages (from accelerate) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/lib/python3.11/site-packages (from accelerate) (23.1)\n",
      "Requirement already satisfied: psutil in /opt/homebrew/lib/python3.11/site-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /opt/homebrew/lib/python3.11/site-packages (from accelerate) (6.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/homebrew/lib/python3.11/site-packages (from accelerate) (2.3.0)\n",
      "Requirement already satisfied: huggingface-hub in /opt/homebrew/lib/python3.11/site-packages (from accelerate) (0.23.4)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/homebrew/lib/python3.11/site-packages (from accelerate) (0.4.3)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/homebrew/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (4.11.0)\n",
      "Requirement already satisfied: sympy in /opt/homebrew/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/homebrew/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/homebrew/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
      "Requirement already satisfied: requests in /opt/homebrew/lib/python3.11/site-packages (from huggingface-hub->accelerate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/homebrew/lib/python3.11/site-packages (from huggingface-hub->accelerate) (4.65.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/lib/python3.11/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate) (2023.5.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/homebrew/lib/python3.11/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip uninstall transformers -y #uninstall version you have currently, just bc we might need diff versions for diff fine tuning models\n",
    "#!pip install transformers==4.18.0 #note that we need an older version of transformers to use this model\n",
    "!pip install transformers \n",
    "!pip install --no-cache-dir transformers sentencepiece\n",
    "!pip install datasets\n",
    "!pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from datasets import Dataset, load_metric\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from google.colab import files\n",
    "from google.colab import drive\n",
    "import glob\n",
    "import zipfile\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    metric = load_metric(\"accuracy\")\n",
    "    logits, labels = p\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the tokenizer for the french model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"osiria/bert-tweet-italian-uncased-sentiment\")\n",
    "\n",
    "\n",
    "def tokenize_function(examples, tokenizer=tokenizer):\n",
    "    \"\"\"\n",
    "    Function to tokenize the data.\n",
    "    examples : data to tokenize ; dict\n",
    "    tokenizer : tokenizer to use ; DistilBertTokenizer\n",
    "    \"\"\"\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(train_comments, train_labels,\n",
    "                    val_comments, val_labels, test_comments,\n",
    "                    test_labels,\n",
    "                    batch_size_train, batch_size_val,\n",
    "                    epochs, tokenizer=tokenizer, learning_rate=1e-4, weight_decay=0.001):\n",
    "    \"\"\"\n",
    "    Function to train a  model on the data.\n",
    "    train_comments : comments for training ; lst of str\n",
    "    train_labels : labels for training ; lst of int\n",
    "    val_comments : comments for validation ; lst of str\n",
    "    val_labels : labels for validation ; lst of int\n",
    "    batch_size_train : batch size for training ; int\n",
    "    batch_size_val : batch size for validation ; int\n",
    "    epochs : number of epochs ; int\n",
    "    tokenizer : tokenizer to use\n",
    "    \"\"\"\n",
    "    # Set the device to GPU if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"osiria/bert-tweet-italian-uncased-sentiment\")\n",
    "    model.to(device) # French model has no gpu acceleration\n",
    "\n",
    "    # Setup the Hugging Face Dataset Class\n",
    "    train_dataset_dict = {\"text\": train_comments, \"label\": train_labels}\n",
    "    val_dataset_dict = {\"text\": val_comments, \"label\": val_labels}\n",
    "    test_dataset_dict = {\"text\": test_comments, \"label\": test_labels}\n",
    "\n",
    "    train_dataset = Dataset.from_dict(train_dataset_dict)\n",
    "    val_dataset = Dataset.from_dict(val_dataset_dict)\n",
    "    test_dataset = Dataset.from_dict(test_dataset_dict)\n",
    "\n",
    "    # Apply the tokenizer to the datasets\n",
    "    train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "    val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "    test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "    # Remove columns we do not need for training\n",
    "    train_dataset = train_dataset.remove_columns([\"text\"])\n",
    "    val_dataset = val_dataset.remove_columns([\"text\"])\n",
    "    test_dataset = test_dataset.remove_columns([\"text\"])\n",
    "\n",
    "    # Set the format of the datasets to PyTorch tensors\n",
    "    train_dataset.set_format(\"torch\")\n",
    "    val_dataset.set_format(\"torch\")\n",
    "    test_dataset.set_format(\"torch\")\n",
    "\n",
    "\n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./results',          # output directory\n",
    "        num_train_epochs=epochs,              # total number of training epochs\n",
    "        per_device_train_batch_size=batch_size_train,  # batch size for training\n",
    "        per_device_eval_batch_size=batch_size_val,   # batch size for evaluation\n",
    "        warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "        weight_decay=weight_decay,               # strength of weight decay\n",
    "        logging_dir='./logs',            # directory for storing logs\n",
    "        logging_steps=10,\n",
    "        evaluation_strategy=\"steps\",     # Evaluate every `eval_steps`\n",
    "        eval_steps=10,                   # Number of steps between evaluations\n",
    "        save_steps=10,                   # Save the model every `save_steps`\n",
    "        load_best_model_at_end=True,     # Load the best model at the end of training#\n",
    "        learning_rate= learning_rate,              # Set the learning rate\n",
    "        metric_for_best_model=\"eval_loss\", # Use evaluation loss to check how good our model is performing\n",
    "        greater_is_better=False,\n",
    "    )\n",
    "\n",
    "    # Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,                         # model\n",
    "        args=training_args,                  # training arguments, defined above\n",
    "        train_dataset=train_dataset,         # training dataset\n",
    "        eval_dataset=val_dataset,            # evaluation dataset\n",
    "        tokenizer=tokenizer,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=10)], # Early Stopping for Overfitting\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    test_results = trainer.evaluate(eval_dataset=test_dataset)\n",
    "\n",
    "    # Get test accuracy\n",
    "    print(\"Test Accuracy:\", test_results['eval_accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "    return model, tokenizer, test_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, tokenizer, path):\n",
    "    \"\"\"\n",
    "    Function to save the model\n",
    "    model : model to save ;\n",
    "    tokenizer : tokenizer to save ;\n",
    "    path : path to save the model ; str\n",
    "    \"\"\"\n",
    "\n",
    "    model_save_path =  path\n",
    "    model.save_pretrained(model_save_path)\n",
    "    tokenizer.save_pretrained(model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open a file upload dialog\n",
    "# Select here all files to upload!\n",
    "# If already uploaded, just press 'Cancel Upload'\n",
    "# They are contained within a zip file.\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path to the data\n",
    "# On local machine use the relative path, for example\n",
    "# path = 'NLP labelled data preview/english set/'\n",
    "# On Google Colab use this path\n",
    "# '/content/'\n",
    "path = '/content/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzip the folder\n",
    "with zipfile.ZipFile(path + 'italian_data_finetuning.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('italian_data_finetuning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "all_comments_with_labels = glob.glob(path + 'italian_data_finetuning/italian_data_finetuning/*.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "all_comments_with_labels = pd.concat([pd.read_csv(f) for f in all_comments_with_labels], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : remove, just testing purposes on local machine\n",
    "all_comments_with_labels = pd.read_csv('/Users/marlon/VS-Code-Projects/Youtube/Comments DB/Italian/Finetuning/TEST_ITALIAN.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# FOR TESTING PURPOSES (since we don't have the labels for french yet)\n",
    "# Add a new column to the dataframe all_comments_with_labels and fill it up with integer values randomly distributed between 0 and 2\n",
    "all_comments_with_labels['label'] = [random.randint(0, 1) for i in range(len(all_comments_with_labels))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion to labels of the model we fine-tune\n",
    "# model stores 0 as negative, 1 as neutral and 2 as positive\n",
    "all_comments_with_labels['label'] = all_comments_with_labels['label'].replace({0: 0, 1: 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperate the two columns in the dataframe into comments and labels and turn them into lists\n",
    "comments = all_comments_with_labels['Comment'].tolist()\n",
    "labels = all_comments_with_labels['label'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the labels to integers\n",
    "labels = [int(label) for label in labels]\n",
    "# Turn all elements in comments into strings\n",
    "comments = [str(comment) for comment in comments]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of comments:  3907\n",
      "Number of negative comments:  1956\n",
      "Number of neutral comments:  1951\n"
     ]
    }
   ],
   "source": [
    "# Assess distributions\n",
    "print(\"Number of comments: \", len(comments))\n",
    "print(\"Number of negative comments: \", labels.count(0))\n",
    "print(\"Number of neutral comments: \", labels.count(2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into a train and test set with stratification\n",
    "train_comments, temp_comments, train_labels, temp_labels = train_test_split(\n",
    "    comments, labels, test_size=0.3, random_state=42, stratify=labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and validation sets with stratification\n",
    "val_comments, test_comments, val_labels, test_labels = train_test_split(temp_comments, temp_labels, test_size=0.1, random_state=42, stratify= temp_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The OrderedVocab you are attempting to save contains holes for indices [9, 12, 15, 16, 25, 29, 33, 38, 51, 59, 81, 197, 262, 314, 389, 404, 558, 561, 568, 634, 745, 917, 1143, 1683, 1971, 3283, 3341, 4052], your vocabulary could be corrupted !\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "672aa0ee365440b7964be9a686735812",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2734 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The OrderedVocab you are attempting to save contains holes for indices [9, 12, 15, 16, 25, 29, 33, 38, 51, 59, 81, 197, 262, 314, 389, 404, 558, 561, 568, 634, 745, 917, 1143, 1683, 1971, 3283, 3341, 4052], your vocabulary could be corrupted !\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14a79831afc64b68b49b69f5509988b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1055 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The OrderedVocab you are attempting to save contains holes for indices [9, 12, 15, 16, 25, 29, 33, 38, 51, 59, 81, 197, 262, 314, 389, 404, 558, 561, 568, 634, 745, 917, 1143, 1683, 1971, 3283, 3341, 4052], your vocabulary could be corrupted !\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71a164770c3446d7a5e084787b611570",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/118 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc3f3d0536c74d1cad77cf7c99853d9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/171 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertSdpaSelfAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support non-absolute `position_embedding_type` or `output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5793, 'grad_norm': 23.897062301635742, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.06}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30e36ea87da3404590920796b2b81387",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/132 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sk/0h9rz2217_3gnr24gbjgn6900000gn/T/ipykernel_18880/1435467894.py:2: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"accuracy\")\n",
      "/opt/homebrew/lib/python3.11/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.1/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3808104395866394, 'eval_accuracy': 0.36492890995260663, 'eval_runtime': 8.4916, 'eval_samples_per_second': 124.24, 'eval_steps_per_second': 15.545, 'epoch': 0.06}\n",
      "The OrderedVocab you are attempting to save contains holes for indices [9, 12, 15, 16, 25, 29, 33, 38, 51, 59, 81, 197, 262, 314, 389, 404, 558, 561, 568, 634, 745, 917, 1143, 1683, 1971, 3283, 3341, 4052], your vocabulary could be corrupted !\n",
      "{'loss': 0.2002, 'grad_norm': 14.385143280029297, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.12}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f976bb76a3e4e94845bceab9b86b52b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/132 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.1/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.17519886791706085, 'eval_accuracy': 0.42274881516587676, 'eval_runtime': 6.7832, 'eval_samples_per_second': 155.531, 'eval_steps_per_second': 19.46, 'epoch': 0.12}\n",
      "The OrderedVocab you are attempting to save contains holes for indices [9, 12, 15, 16, 25, 29, 33, 38, 51, 59, 81, 197, 262, 314, 389, 404, 558, 561, 568, 634, 745, 917, 1143, 1683, 1971, 3283, 3341, 4052], your vocabulary could be corrupted !\n",
      "{'loss': 0.0773, 'grad_norm': 2.75414776802063, 'learning_rate': 6e-06, 'epoch': 0.18}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "525ff91bee7b48f2aa4887a88874185b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/132 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.1/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.04268937557935715, 'eval_accuracy': 0.48530805687203793, 'eval_runtime': 6.4746, 'eval_samples_per_second': 162.944, 'eval_steps_per_second': 20.387, 'epoch': 0.18}\n",
      "The OrderedVocab you are attempting to save contains holes for indices [9, 12, 15, 16, 25, 29, 33, 38, 51, 59, 81, 197, 262, 314, 389, 404, 558, 561, 568, 634, 745, 917, 1143, 1683, 1971, 3283, 3341, 4052], your vocabulary could be corrupted !\n",
      "{'loss': 0.0153, 'grad_norm': 0.02131364494562149, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.23}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce47738f14fa4978b20f852984e72e91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/132 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.1/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.006227274425327778, 'eval_accuracy': 0.4985781990521327, 'eval_runtime': 6.2555, 'eval_samples_per_second': 168.651, 'eval_steps_per_second': 21.101, 'epoch': 0.23}\n",
      "The OrderedVocab you are attempting to save contains holes for indices [9, 12, 15, 16, 25, 29, 33, 38, 51, 59, 81, 197, 262, 314, 389, 404, 558, 561, 568, 634, 745, 917, 1143, 1683, 1971, 3283, 3341, 4052], your vocabulary could be corrupted !\n",
      "{'loss': 0.0013, 'grad_norm': 0.4061475992202759, 'learning_rate': 1e-05, 'epoch': 0.29}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ce7cd6f999c42729b56c0c274dba642",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/132 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.1/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0008363722590729594, 'eval_accuracy': 0.500473933649289, 'eval_runtime': 6.2904, 'eval_samples_per_second': 167.717, 'eval_steps_per_second': 20.985, 'epoch': 0.29}\n",
      "The OrderedVocab you are attempting to save contains holes for indices [9, 12, 15, 16, 25, 29, 33, 38, 51, 59, 81, 197, 262, 314, 389, 404, 558, 561, 568, 634, 745, 917, 1143, 1683, 1971, 3283, 3341, 4052], your vocabulary could be corrupted !\n",
      "{'loss': 0.0008, 'grad_norm': 0.007836715318262577, 'learning_rate': 1.2e-05, 'epoch': 0.35}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69232a2d17c44b6296b8186c65b541d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/132 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.1/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.00041940799565054476, 'eval_accuracy': 0.500473933649289, 'eval_runtime': 6.2841, 'eval_samples_per_second': 167.884, 'eval_steps_per_second': 21.005, 'epoch': 0.35}\n",
      "The OrderedVocab you are attempting to save contains holes for indices [9, 12, 15, 16, 25, 29, 33, 38, 51, 59, 81, 197, 262, 314, 389, 404, 558, 561, 568, 634, 745, 917, 1143, 1683, 1971, 3283, 3341, 4052], your vocabulary could be corrupted !\n",
      "{'loss': 0.0004, 'grad_norm': 0.012791426852345467, 'learning_rate': 1.4000000000000001e-05, 'epoch': 0.41}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2132a9d2bd7422bbedc765ca7983cea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/132 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.1/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0002989808563143015, 'eval_accuracy': 0.500473933649289, 'eval_runtime': 6.2991, 'eval_samples_per_second': 167.485, 'eval_steps_per_second': 20.955, 'epoch': 0.41}\n",
      "The OrderedVocab you are attempting to save contains holes for indices [9, 12, 15, 16, 25, 29, 33, 38, 51, 59, 81, 197, 262, 314, 389, 404, 558, 561, 568, 634, 745, 917, 1143, 1683, 1971, 3283, 3341, 4052], your vocabulary could be corrupted !\n",
      "{'loss': 0.0005, 'grad_norm': 0.019003180786967278, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.47}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0497e7cc8aa45a793be22c685787502",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/132 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.1/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.00023443963436875492, 'eval_accuracy': 0.500473933649289, 'eval_runtime': 6.3083, 'eval_samples_per_second': 167.241, 'eval_steps_per_second': 20.925, 'epoch': 0.47}\n",
      "The OrderedVocab you are attempting to save contains holes for indices [9, 12, 15, 16, 25, 29, 33, 38, 51, 59, 81, 197, 262, 314, 389, 404, 558, 561, 568, 634, 745, 917, 1143, 1683, 1971, 3283, 3341, 4052], your vocabulary could be corrupted !\n",
      "{'loss': 0.0005, 'grad_norm': 0.020320503041148186, 'learning_rate': 1.8e-05, 'epoch': 0.53}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da16257b8c94438aaff6b31c0502f283",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/132 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.1/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0001916384935611859, 'eval_accuracy': 0.500473933649289, 'eval_runtime': 6.5665, 'eval_samples_per_second': 160.663, 'eval_steps_per_second': 20.102, 'epoch': 0.53}\n",
      "The OrderedVocab you are attempting to save contains holes for indices [9, 12, 15, 16, 25, 29, 33, 38, 51, 59, 81, 197, 262, 314, 389, 404, 558, 561, 568, 634, 745, 917, 1143, 1683, 1971, 3283, 3341, 4052], your vocabulary could be corrupted !\n",
      "{'loss': 0.0002, 'grad_norm': 0.019119389355182648, 'learning_rate': 2e-05, 'epoch': 0.58}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5889905f050a450e81716d87b084a4d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/132 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.1/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0001588244194863364, 'eval_accuracy': 0.500473933649289, 'eval_runtime': 6.3014, 'eval_samples_per_second': 167.423, 'eval_steps_per_second': 20.948, 'epoch': 0.58}\n",
      "The OrderedVocab you are attempting to save contains holes for indices [9, 12, 15, 16, 25, 29, 33, 38, 51, 59, 81, 197, 262, 314, 389, 404, 558, 561, 568, 634, 745, 917, 1143, 1683, 1971, 3283, 3341, 4052], your vocabulary could be corrupted !\n",
      "{'loss': 0.0003, 'grad_norm': 0.006308269686996937, 'learning_rate': 2.2000000000000003e-05, 'epoch': 0.64}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21778fa3dee44306897c2b54d2c2e99e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/132 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.1/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.00013039256737101823, 'eval_accuracy': 0.500473933649289, 'eval_runtime': 6.5846, 'eval_samples_per_second': 160.223, 'eval_steps_per_second': 20.047, 'epoch': 0.64}\n",
      "The OrderedVocab you are attempting to save contains holes for indices [9, 12, 15, 16, 25, 29, 33, 38, 51, 59, 81, 197, 262, 314, 389, 404, 558, 561, 568, 634, 745, 917, 1143, 1683, 1971, 3283, 3341, 4052], your vocabulary could be corrupted !\n",
      "{'loss': 0.0002, 'grad_norm': 0.00529207568615675, 'learning_rate': 2.4e-05, 'epoch': 0.7}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52957e0b4e634a94a104b5d6105ef28a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/132 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.1/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.00010894491424551234, 'eval_accuracy': 0.500473933649289, 'eval_runtime': 7.3868, 'eval_samples_per_second': 142.822, 'eval_steps_per_second': 17.87, 'epoch': 0.7}\n",
      "The OrderedVocab you are attempting to save contains holes for indices [9, 12, 15, 16, 25, 29, 33, 38, 51, 59, 81, 197, 262, 314, 389, 404, 558, 561, 568, 634, 745, 917, 1143, 1683, 1971, 3283, 3341, 4052], your vocabulary could be corrupted !\n",
      "{'loss': 0.0003, 'grad_norm': 0.007099503185600042, 'learning_rate': 2.6000000000000002e-05, 'epoch': 0.76}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bdad04010b94873a788ac9be68c59b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/132 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.1/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 9.237420454155654e-05, 'eval_accuracy': 0.500473933649289, 'eval_runtime': 6.329, 'eval_samples_per_second': 166.693, 'eval_steps_per_second': 20.856, 'epoch': 0.76}\n",
      "The OrderedVocab you are attempting to save contains holes for indices [9, 12, 15, 16, 25, 29, 33, 38, 51, 59, 81, 197, 262, 314, 389, 404, 558, 561, 568, 634, 745, 917, 1143, 1683, 1971, 3283, 3341, 4052], your vocabulary could be corrupted !\n",
      "{'loss': 0.0002, 'grad_norm': 0.00301172467879951, 'learning_rate': 2.8000000000000003e-05, 'epoch': 0.82}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2fd16ae01554caf8f5d773bbe20d810",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/132 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.1/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 7.824739441275597e-05, 'eval_accuracy': 0.500473933649289, 'eval_runtime': 6.3019, 'eval_samples_per_second': 167.409, 'eval_steps_per_second': 20.946, 'epoch': 0.82}\n",
      "The OrderedVocab you are attempting to save contains holes for indices [9, 12, 15, 16, 25, 29, 33, 38, 51, 59, 81, 197, 262, 314, 389, 404, 558, 561, 568, 634, 745, 917, 1143, 1683, 1971, 3283, 3341, 4052], your vocabulary could be corrupted !\n",
      "{'loss': 0.0001, 'grad_norm': 0.0020587800536304712, 'learning_rate': 3e-05, 'epoch': 0.88}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3ebfda2e45b4624b8b8c8bc54351290",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/132 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.1/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.878768908791244e-05, 'eval_accuracy': 0.500473933649289, 'eval_runtime': 6.4718, 'eval_samples_per_second': 163.016, 'eval_steps_per_second': 20.396, 'epoch': 0.88}\n",
      "The OrderedVocab you are attempting to save contains holes for indices [9, 12, 15, 16, 25, 29, 33, 38, 51, 59, 81, 197, 262, 314, 389, 404, 558, 561, 568, 634, 745, 917, 1143, 1683, 1971, 3283, 3341, 4052], your vocabulary could be corrupted !\n",
      "{'loss': 0.0001, 'grad_norm': 0.0024895912501960993, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.94}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47a34c620f5b46c5acb4d5a28c915698",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/132 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.1/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.049883450032212e-05, 'eval_accuracy': 0.500473933649289, 'eval_runtime': 6.7701, 'eval_samples_per_second': 155.831, 'eval_steps_per_second': 19.497, 'epoch': 0.94}\n",
      "The OrderedVocab you are attempting to save contains holes for indices [9, 12, 15, 16, 25, 29, 33, 38, 51, 59, 81, 197, 262, 314, 389, 404, 558, 561, 568, 634, 745, 917, 1143, 1683, 1971, 3283, 3341, 4052], your vocabulary could be corrupted !\n",
      "{'loss': 0.0001, 'grad_norm': 0.002032345626503229, 'learning_rate': 3.4000000000000007e-05, 'epoch': 0.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2af9e1f6feec4b259c1f206b5ed5dd9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/132 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.1/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 5.318586772773415e-05, 'eval_accuracy': 0.500473933649289, 'eval_runtime': 6.2946, 'eval_samples_per_second': 167.604, 'eval_steps_per_second': 20.97, 'epoch': 0.99}\n",
      "The OrderedVocab you are attempting to save contains holes for indices [9, 12, 15, 16, 25, 29, 33, 38, 51, 59, 81, 197, 262, 314, 389, 404, 558, 561, 568, 634, 745, 917, 1143, 1683, 1971, 3283, 3341, 4052], your vocabulary could be corrupted !\n",
      "{'train_runtime': 207.787, 'train_samples_per_second': 13.158, 'train_steps_per_second': 0.823, 'train_loss': 0.05128877933733407, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1849cfef08284fbcb2a48198f39782b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.1/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model_trained, tokenizer_trained, test_results = model(train_comments, train_labels, val_comments, val_labels,test_comments, test_labels, batch_size_train = 16, batch_size_val = 8, epochs = 1, tokenizer = tokenizer, learning_rate=1e-4, weight_decay=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "save_model(model_trained, tokenizer_trained, path + 'italian_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The OrderedVocab you are attempting to save contains holes for indices [9, 12, 15, 16, 25, 29, 33, 38, 51, 59, 81, 197, 262, 314, 389, 404, 558, 561, 568, 634, 745, 917, 1143, 1683, 1971, 3283, 3341, 4052], your vocabulary could be corrupted !\n"
     ]
    }
   ],
   "source": [
    "# TODO : remove, just for local\n",
    "\n",
    "save_model(model_trained, tokenizer_trained, '/Users/marlon/VS-Code-Projects/Youtube/sentiment_model_finetuned_italian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model to your system\n",
    "\n",
    "# Mount to drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "!cp -r /content/italian_model /content/drive/MyDrive/\n",
    "\n",
    "# Now download it from your Google Drive Account !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
