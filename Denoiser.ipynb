{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets\n",
    "!pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import regex as re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from google.colab import files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(comment):\n",
    "    \"\"\"Function to remove emojis.\n",
    "        comment : data input ; str\n",
    "        Taken from :\n",
    "        https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\U00010000-\\U0010ffff\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\ufe0f\"  # dingbats\n",
    "                               u\"\\u3030\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def P_data_cleaning(data, language, labelling):\n",
    "    \"\"\"Function to clean our data.\n",
    "       data : data input ; pd.Series\n",
    "       language : what language the comments are in (input in lowercase) : str\n",
    "       labelling : if we want to label, we keep punctuation & stopwords\n",
    "    \"\"\"\n",
    "\n",
    "    # REMOVE NAN ENTRIES\n",
    "    data = data.dropna()\n",
    "\n",
    "    # REMOVE COMMENTS THAT EXCEED CERTAIN LENGTH (350 for now)\n",
    "    data = data[data.str.len() <= 350]\n",
    "    \n",
    "\n",
    "    # FOR GERMAN DATA : Change ö , ä , ü to oe, ae, ue\n",
    "    data = data.str.replace(\"ö\", \"oe\").str.replace(\"ä\", \"ae\").str.replace(\"ü\", \"ue\")\n",
    "\n",
    "    # REMOVE NAMES FROM ANSWERS (in youtube comments scraper answers stored by @@)\n",
    "    data = data.str.replace('@@\\w+', '', regex=True)\n",
    "\n",
    "    # REMOVING PUNCTUATION\n",
    "    if labelling == False:\n",
    "      data = data.str.replace('[^a-zA-Z0-9]',' ')\n",
    "\n",
    "    # REMOVING EMOJIS\n",
    "    data = data.apply(lambda x: remove_emoji(x))\n",
    "\n",
    "    # LOWERCASE\n",
    "    data = data.str.lower()\n",
    "\n",
    "    # REMOVING STOPWORDS\n",
    "    if labelling == False:\n",
    "      data = data.apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords.words(language))]))\n",
    "\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "\n",
    "def tokenize_function(examples, tokenizer=tokenizer):\n",
    "    \"\"\"\n",
    "    Function to tokenize the data.\n",
    "    examples : data to tokenize ; dict\n",
    "    tokenizer : tokenizer to use ; DistilBertTokenizer\n",
    "    \"\"\"\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DistilBertModel(train_comments, train_labels, \n",
    "                    val_comments, val_labels,\n",
    "                    batch_size_train, batch_size_val,\n",
    "                    epochs, num_labels, tokenizer=tokenizer):\n",
    "    \"\"\"\n",
    "    Function to train a DistilBert model on the data.\n",
    "    train_comments : comments for training ; lst of str\n",
    "    train_labels : labels for training ; lst of int\n",
    "    val_comments : comments for validation ; lst of str\n",
    "    val_labels : labels for validation ; lst of int\n",
    "    batch_size_train : batch size for training ; int\n",
    "    batch_size_val : batch size for validation ; int\n",
    "    epochs : number of epochs ; int\n",
    "    num_labels : number of labels (for denoiser 2, for classification 3) ; int\n",
    "    tokenizer : tokenizer to use ; DistilBertTokenizer\n",
    "    \"\"\"\n",
    "\n",
    "    model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=num_labels)\n",
    "    \n",
    "\n",
    "    # Setup the Hugging Face Dataset Class\n",
    "    train_dataset_dict = {\"text\": train_comments, \"label\": train_labels}\n",
    "    val_dataset_dict = {\"text\": val_comments, \"label\": val_labels}\n",
    "\n",
    "    train_dataset = Dataset.from_dict(train_dataset_dict)\n",
    "    val_dataset = Dataset.from_dict(val_dataset_dict)\n",
    "\n",
    "    # Apply the tokenizer to the datasets\n",
    "    train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "    val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "    # Remove columns we do not need for training\n",
    "    train_dataset = train_dataset.remove_columns([\"text\"])\n",
    "    val_dataset = val_dataset.remove_columns([\"text\"])\n",
    "\n",
    "    # Set the format of the datasets to PyTorch tensors\n",
    "    train_dataset.set_format(\"torch\")\n",
    "    val_dataset.set_format(\"torch\")\n",
    "\n",
    "\n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./results',          # output directory\n",
    "        num_train_epochs=epochs,              # total number of training epochs\n",
    "        per_device_train_batch_size=batch_size_train,  # batch size for training\n",
    "        per_device_eval_batch_size=batch_size_val,   # batch size for evaluation\n",
    "        warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "        weight_decay=0.01,               # strength of weight decay\n",
    "        logging_dir='./logs',            # directory for storing logs\n",
    "        logging_steps=10,\n",
    "    )\n",
    "\n",
    "    # Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,                         # model\n",
    "        args=training_args,                  # training arguments, defined above\n",
    "        train_dataset=train_dataset,         # training dataset\n",
    "        eval_dataset=val_dataset,            # evaluation dataset\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "\n",
    "\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, tokenizer, path):\n",
    "    \"\"\"\n",
    "    Function to save the model\n",
    "    model : model to save ; DistilBertForSequenceClassification\n",
    "    tokenizer : tokenizer to save ; DistilBertTokenizer\n",
    "    path : path to save the model ; str\n",
    "    \"\"\"\n",
    "\n",
    "    model_save_path =  path\n",
    "    model.save_pretrained(model_save_path)\n",
    "    tokenizer.save_pretrained(model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open a file upload dialog\n",
    "# Select here all files to upload!\n",
    "# If already uploaded, just press 'Cancel Upload'\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path to the data\n",
    "# On local machine use the relative path, for example\n",
    "# path = 'NLP labelled data preview/english set/'\n",
    "path = '/content/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "# Note that mac users seperated with , automatically, for windows users we have to specify ; as the seperator\n",
    "english_test_dataset_labelled = pd.read_csv(path +'Giuseppe.csv')\n",
    "english_test_dataset_labelled_2 = pd.read_csv(path + 'andrea.csv', sep= ';')\n",
    "english_test_dataset_labelled_3 = pd.read_csv(path + 'giovanni.csv', sep= ';')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the datasets\n",
    "english_test_dataset_labelled = pd.concat([english_test_dataset_labelled, english_test_dataset_labelled_2, english_test_dataset_labelled_3], ignore_index= True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove comments that are not labelled (have NaN value in column 'Label')\n",
    "english_test_dataset_labelled = english_test_dataset_labelled.dropna(subset=['Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do preprocessing steps\n",
    "english_test_dataset_labelled['Comment'] = P_data_cleaning(english_test_dataset_labelled['Comment'], 'english', False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperate the two columns in the dataframe into 'comment' and 'label' in form of two lists\n",
    "english_test_dataset_labelled_comments = english_test_dataset_labelled['Comment'].tolist()\n",
    "english_test_dataset_labelled_labels = english_test_dataset_labelled['Label'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now seperate into noise and no noise\n",
    "english_test_dataset_labelled_comments_noise = []\n",
    "\n",
    "# We will refer to no noise as 0 and noise as 1. Turn every entry in english_test_dataset_labelled_labels into 1 if it is 'N', else into 0.\n",
    "english_test_dataset_labelled_labels = [1 if label == 'N' else 0 for label in english_test_dataset_labelled_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the amount of noisy and non-noisy comments\n",
    "print('Amount of noisy comments:', english_test_dataset_labelled_labels.count(1))\n",
    "print('Amount of non-noisy comments:', english_test_dataset_labelled_labels.count(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For easy testing purposes, I'll just take as much noisy comments as non-noisy comments. \n",
    "# If we label more, we could think about stuff like data augmentation (synonym replacement, etc.)\n",
    "# Else, oversampling the noisy comments would be a good idea. But care for overfitting.\n",
    "\n",
    "# Seperate noisy and non-noisy comments\n",
    "english_test_dataset_labelled_comments_no_noise = []\n",
    "english_test_dataset_labelled_comments_noise = []\n",
    "\n",
    "for i in range(len(english_test_dataset_labelled_labels)):\n",
    "    if english_test_dataset_labelled_labels[i] == 0:\n",
    "        english_test_dataset_labelled_comments_no_noise.append(english_test_dataset_labelled_comments[i])\n",
    "    else:\n",
    "        english_test_dataset_labelled_comments_noise.append(english_test_dataset_labelled_comments[i])\n",
    "\n",
    "# Take the first len(english_test_dataset_labelled_comments_noise) comments from the noisy comments\n",
    "english_test_dataset_labelled_comments_no_noise = english_test_dataset_labelled_comments_no_noise[:len(english_test_dataset_labelled_comments_noise)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the amount of noisy and non-noisy comments is the same\n",
    "print('Amount of noisy comments:', len(english_test_dataset_labelled_comments_noise))\n",
    "print('Amount of non-noisy comments:', len(english_test_dataset_labelled_comments_no_noise))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the comments and labels into a single list\n",
    "comments = english_test_dataset_labelled_comments_no_noise + english_test_dataset_labelled_comments_noise\n",
    "labels = [0]*len(english_test_dataset_labelled_comments_no_noise) + [1]*len(english_test_dataset_labelled_comments_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and validation sets with stratification\n",
    "train_comments, val_comments, train_labels, val_labels = train_test_split(\n",
    "    comments, labels, test_size=0.2, random_state=42, stratify=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the amount of training and validation comments\n",
    "print(\"We have {} training comments\".format(len(train_comments)))\n",
    "print(\"We have {} validation comments\".format(len(val_comments)))\n",
    "\n",
    "# Check that the classes are evenly distributed across training and validation sets\n",
    "print(\"Training set:\")\n",
    "print(\"Noisy comments:\", train_labels.count(1))\n",
    "print(\"Non noisy comments:\", train_labels.count(0))\n",
    "print(\"Validation set:\")\n",
    "print(\"Noisy comments:\", val_labels.count(1))\n",
    "print(\"Non noisy comments:\", val_labels.count(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using our evened out dataset, we can start applying the model\n",
    "model_trained, tokenizer_trained = DistilBertModel(train_comments, train_labels, val_comments, val_labels, batch_size_train = 16, batch_size_val = 16, num_labels = 2, epochs = 1, tokenizer = tokenizer)\n",
    "save_model(model_trained, tokenizer_trained, \"denoising_model_fine_tuned_distilbert_english\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
