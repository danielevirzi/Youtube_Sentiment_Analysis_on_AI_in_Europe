{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Secondary cleaning of english comments\n",
        "This File is used to apply our secondary cleaning phase. This is based on what we marked as problematic for our sentiment analysis, based on looking through scraped english comments. In particular, we note that we already did some cleaning steps while scraping. Yet, after scraping, we found some problems looking through the comments. These problems are adressed here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install textblob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3GatpmKEhWiL"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import regex as re\n",
        "import glob\n",
        "from textblob import TextBlob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read in single file\n",
        "all_english_comments = pd.read_csv('Comments DB/english/Scraped/english_processed_full_unlabelled_uncleaned.csv')\n",
        "all_english_comments_original = pd.read_csv('Comments DB/english/Scraped/english_original_full_unlabelled_uncleaned.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "W_N2JOnUhWiO"
      },
      "outputs": [],
      "source": [
        "# Remove comments with words like \"video\" and \"channel\" as they are associated with comments such as 'great video!'\n",
        "# Note we also need to remove from original, because in this case we are removing whole comments !\n",
        "all_english_comments = all_english_comments[~all_english_comments['Comment'].str.contains('video|channel', case=False)]\n",
        "all_english_comments_original = all_english_comments_original[~all_english_comments_original['Comment'].str.contains('video|channel', case=False)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "1VAdrs7yhWiO"
      },
      "outputs": [],
      "source": [
        "# Remove numbers from comments\n",
        "all_english_comments['Comment'] = all_english_comments['Comment'].apply(lambda x: re.sub(r'\\d+', '', x))\n",
        "all_english_comments_original['Comment'] = all_english_comments_original['Comment'].apply(lambda x: re.sub(r'\\d+', '', x))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "CoVZ6rCNhWiO"
      },
      "outputs": [],
      "source": [
        "# Remove words that have the scheme '@something' (i.e. remove words that begin with '@')\n",
        "all_english_comments['Comment'] = all_english_comments['Comment'].apply(lambda x: re.sub(r'@\\w+', '', x))\n",
        "all_english_comments_original['Comment'] = all_english_comments_original['Comment'].apply(lambda x: re.sub(r'@\\w+', '', x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "420bbM7yhWiO"
      },
      "outputs": [],
      "source": [
        "# Remove all special characters except for ., ,, ?, !, /, (, and )\n",
        "all_english_comments['Comment'] = all_english_comments['Comment'].apply(lambda x: re.sub(r'[^a-zA-Z0-9\\s.,?!/()]', '', x))\n",
        "\n",
        "# Replace ., /, (, and ) with whitespace\n",
        "all_english_comments['Comment'] = all_english_comments['Comment'].apply(lambda x: re.sub(r'[.,/()]', ' ', x))\n",
        "\n",
        "# Remove trailing and excessive whitespaces\n",
        "all_english_comments['Comment'] = all_english_comments['Comment'].apply(lambda x: re.sub(r'\\s+', ' ', x).strip())\n",
        "all_english_comments_original['Comment'] = all_english_comments_original['Comment'].apply(lambda x: re.sub(r'\\s+', ' ', x).strip())\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# We also aim to fix spelling mistakes, as they can mess up our sentiment analysis heavily (we tested that on some models and got completely wrong sentiments due to spelling mistakes)\n",
        "# This we can also do on the original comments directly \n",
        "# This will later on also be crucial for translation into other languages !\n",
        "\n",
        "# Function to correct spelling mistakes\n",
        "def correct_spelling(text):\n",
        "    try:\n",
        "        corrected_text = str(TextBlob(text).correct())\n",
        "        return corrected_text\n",
        "    except Exception as e:\n",
        "        return text\n",
        "    \n",
        "all_english_comments['Comment'] = all_english_comments['Comment'].apply(correct_spelling)\n",
        "all_english_comments_original['Comment'] = all_english_comments_original['Comment'].apply(correct_spelling)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "XC_2ByzjhWiO"
      },
      "outputs": [],
      "source": [
        "# Dropping duplicates\n",
        "all_english_comments_combined = pd.concat([all_english_comments.reset_index(drop=True), all_english_comments_original.reset_index(drop=True)], axis = 1)\n",
        "all_english_comments_combined.columns = ['Comment processed', 'Comment original']\n",
        "all_english_comments_combined = all_english_comments_combined.drop_duplicates(subset='Comment processed', keep='first')\n",
        "# Split the dataframes again\n",
        "all_english_comments = all_english_comments_combined[['Comment processed']].rename(columns={'Comment processed': 'Comment'})\n",
        "all_english_comments_original = all_english_comments_combined[['Comment original']].rename(columns={'Comment original': 'Comment'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Mn4li3w3hWiO"
      },
      "outputs": [],
      "source": [
        "# Save to csv\n",
        "all_english_comments.to_csv('Comments DB/english/ReadyForLabelling/english_processed_full_unlabelled.csv')\n",
        "all_english_comments_original.to_csv('Comments DB/english/ReadyForLabelling/english_original_full_unlabelled.csv')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
