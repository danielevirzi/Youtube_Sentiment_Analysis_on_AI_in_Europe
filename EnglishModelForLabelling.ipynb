{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "from google.colab import files\n",
    "from google.colab import drive\n",
    "import glob\n",
    "import zipfile\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open a file upload dialog\n",
    "# Select here all files to upload!\n",
    "# If already uploaded, just press 'Cancel Upload'\n",
    "# Note that we are here uploading all the english comments that we have past filtering phase.\n",
    "# They are contained within a zip file.\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path to the data \n",
    "# On local machine use the relative path, for example\n",
    "# path = 'NLP labelled data preview/english set/'\n",
    "# On Google Colab use this path \n",
    "# '/content/'\n",
    "path = '/content/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzip the folder\n",
    "with zipfile.ZipFile(path + 'english_data.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('english_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "all_english_comments = glob.glob(path + 'english_data/english_data/*.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "all_english_comments = pd.concat([pd.read_csv(f) for f in all_english_comments], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : CHANGE LOCATION OF THESE STEPS !! \n",
    "\n",
    "# Remove comments with words like \"video\" and \"channel\" as they are associated with comments such as 'great video!'\n",
    "all_english_comments = all_english_comments[~all_english_comments['Comment'].str.contains('video|channel', case=False)]\n",
    "\n",
    "# Remove comments whose length is less than 3 words\n",
    "all_english_comments = all_english_comments[all_english_comments['Comment'].str.split().str.len() > 3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we prepare for the labelling phase using a pre-trained state-of-the-art model\n",
    "\n",
    "# Turn dataframe into a list\n",
    "comments = all_english_comments['Comment'].tolist()\n",
    "\n",
    "# Turn all comments into strings\n",
    "comments = [str(comment) for comment in comments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the different models, trained on different datasets\n",
    "tokenizer_1 = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
    "model_1 = AutoModelForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
    "\n",
    "# TODO : change to correct model, just for testing purposes\n",
    "tokenizer_2 = AutoTokenizer.from_pretrained(\"aychang/roberta-base-imdb\")\n",
    "model_2 = AutoModelForSequenceClassification.from_pretrained(\"aychang/roberta-base-imdb\")\n",
    "\n",
    "tokenizer_3 = AutoTokenizer.from_pretrained(\"siebert/sentiment-roberta-large-english\")\n",
    "model_3 = AutoModelForSequenceClassification.from_pretrained(\"siebert/sentiment-roberta-large-english\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the models to GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_1.to(device)\n",
    "model_2.to(device)\n",
    "model_3.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the pipelines\n",
    "classifier_1 = pipeline('sentiment-analysis', model=model_1, tokenizer=tokenizer_1)\n",
    "classifier_2 = pipeline('sentiment-analysis', model=model_2, tokenizer=tokenizer_2)\n",
    "classifier_3 = pipeline('sentiment-analysis', model=model_3, tokenizer=tokenizer_3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict sentiment labels for each classifier\n",
    "predictions_1 = classifier_1(comments)\n",
    "predictions_2 = classifier_2(comments)\n",
    "predictions_3 = classifier_3(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the scores from the predictions\n",
    "scores_1 = [prediction['score'] for prediction in predictions_1]\n",
    "scores_2 = [prediction['score'] for prediction in predictions_2]\n",
    "scores_3 = [prediction['score'] for prediction in predictions_3]\n",
    "# Extract the labels from the predictions\n",
    "labels_1 = [prediction['label'] for prediction in predictions_1]\n",
    "labels_2 = [prediction['label'] for prediction in predictions_2]\n",
    "labels_3 = [prediction['label'] for prediction in predictions_3]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the right labels for the different models\n",
    "# We want to transform all labels to the same format ; all should be numbers where 0 is negative, 1 is neutral and 2 is positive\n",
    "# Model 1 gives Negative, Neutral, Positive as labels, so we will transform them to 0, 1, 2\n",
    "labels_1 = [0 if label == 'Negative' else 1 if label == 'Neutral' else 2 for label in labels_1]\n",
    "# Model 2 gives neg and pos as labels, so we will transform them to 0, 2\n",
    "labels_2 = [0 if label == 'neg' else 2 for label in labels_2]\n",
    "# Model 3 gives only POSITIVE, NEGATIVE as labels, so we will transform them to 0, 1\n",
    "labels_3 = [0 if label == 'NEGATIVE' else 2 for label in labels_3]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keep comments with a confidence score of above 0.80\n",
    "conf_score = 0.80\n",
    "high_confidence_predictions_1 = []\n",
    "high_confidence_comments_1 = []\n",
    "high_confidence_comments_2 = []\n",
    "high_confidence_predictions_2 = []\n",
    "high_confidence_predictions_3 = []\n",
    "high_confidence_comments_3 = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For model 1 :\n",
    "for i in range(len(scores_1)):\n",
    "    if scores_1[i] > conf_score:\n",
    "        high_confidence_predictions_1.append(labels_1[i])\n",
    "        high_confidence_comments_1.append(comments[i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For model 2 :\n",
    "for i in range(len(scores_2)):\n",
    "    if scores_2[i] > conf_score:\n",
    "        high_confidence_predictions_2.append(labels_2[i])\n",
    "        high_confidence_comments_2.append(comments[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For model 3 :\n",
    "for i in range(len(scores_3)):\n",
    "    if scores_3[i] > conf_score:\n",
    "        high_confidence_predictions_3.append(labels_3[i])\n",
    "        high_confidence_comments_3.append(comments[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we want to keep only the comments which appear in atleast 2 of the 3 models with high confidence\n",
    "# We will use the intersection of the comments from the 3 models\n",
    "# We will also keep the corresponding labels\n",
    "high_confidence_comments = list(set(high_confidence_comments_1) & set(high_confidence_comments_2) & set(high_confidence_comments_3))\n",
    "high_confidence_labels = [label for i, label in enumerate(high_confidence_predictions_1) if high_confidence_comments_1[i] in high_confidence_comments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many comments are left after filtering by confidence score\n",
    "print(\"We have \", len(high_confidence_labels), \" comments left after filtering by confidence score \" , conf_score , \" .\")\n",
    "\n",
    "# Check how many predictions we have in the respective classes\n",
    "print(\"We have \", high_confidence_labels.count(0), \" negative predictions.\")\n",
    "print(\"We have \", high_confidence_labels.count(1), \" neutral predictions.\")\n",
    "print(\"We have \", high_confidence_labels.count(2), \" positive predictions.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to csv the comments and their label\n",
    "high_confidence_comments_df = pd.DataFrame(high_confidence_comments, columns=['comments'])\n",
    "high_confidence_comments_df['predictions'] = high_confidence_labels\n",
    "high_confidence_comments_df.to_csv(path + \"High_Confidence_Comments_English.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the file to your local machine (from google colab)\n",
    "\n",
    "files.download(path + \"High_Confidence_Comments_English.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
