{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Labelling of English Comments\n",
        "In this file, we utilize 5 state-of-the-art sentiment models found on HuggingFace to label our english comments based on sentiment (negative, neutral and positive). We make use of majority voting, that is we only keep comments that appear in atleast 3 of the 5 models with the same label. To remove noise from our comments (which is given due to the high variablility of comments found on YouTube), we only analyze comments that got a confidence score of 80% or higher in every model. Furthermore, note that we keep track of the original comments at all times as we need to translate and augment on these rather than the fully processed ones later on. In particular, we found no comments to be highly classified as neutral, thus we omitted that class from analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2RQLOqqfTM2C"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "import glob\n",
        "import zipfile\n",
        "from collections import Counter, defaultdict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "jnEipSyoTM2D",
        "outputId": "05f5196f-44a1-4a6f-e15c-ba8033c21fa1"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-c2616d3e-c48a-4981-b6e5-095b8ad6d919\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-c2616d3e-c48a-4981-b6e5-095b8ad6d919\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving comments_original_split_0.csv to comments_original_split_0.csv\n",
            "Saving comments_processed_split_0.csv to comments_processed_split_0.csv\n"
          ]
        }
      ],
      "source": [
        "# Open a file upload dialog\n",
        "# Select here all files to upload!\n",
        "# If already uploaded, just press 'Cancel Upload'\n",
        "# Note that we are here uploading all the english comments that we have past filtering phase\n",
        "# aswell as pre filtering phase. We need to keep the original ones, as we later on\n",
        "# translate them to other languages and have to do specific preprocessing (such as stopwords etc.)\n",
        "# for the respective language to fine tune them for the models.\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "e7oZiiDpTM2E"
      },
      "outputs": [],
      "source": [
        "# Set the path to the data\n",
        "# On local machine use the relative path, for example\n",
        "# path = 'NLP labelled data preview/english set/'\n",
        "# On Google Colab use this path\n",
        "# '/content/'\n",
        "path = '/content/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "r0zV9NI6z85A"
      },
      "outputs": [],
      "source": [
        "# Denote the split you are processing\n",
        "SPLIT = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "W5NKIHq-TM2E"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "# For Mac users : do english_data/english_data/*.csv\n",
        "# For Windows users : do english_data/*.csv\n",
        "all_english_comments = pd.read_csv(path + 'comments_processed_split_{}.csv'.format(SPLIT))\n",
        "all_english_comments_original = pd.read_csv(path + 'comments_original_split_{}.csv'.format(SPLIT))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "2cqjn1_2TM2F"
      },
      "outputs": [],
      "source": [
        "# Now we prepare for the labelling phase using a pre-trained state-of-the-art model\n",
        "\n",
        "# Turn dataframe into a list\n",
        "comments = all_english_comments['Comment'].tolist()\n",
        "comments_original = all_english_comments_original['Comment'].tolist()\n",
        "\n",
        "# Turn all comments into strings\n",
        "comments = [str(comment) for comment in comments]\n",
        "comments_original = [str(comment) for comment in comments_original]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0tsMgwUzsv66",
        "outputId": "3a60a59b-5d91-45d4-d737-fd3ec3c45bed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "We have : 14450 many comments\n"
          ]
        }
      ],
      "source": [
        "# Assure that we have the same amount of comments\n",
        "assert len(comments) == len(comments_original), 'The number of comments is not equal!'\n",
        "print(\"We have : {} many comments\".format(len(comments)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SrrgD3G_TM2F",
        "outputId": "646b3acb-7c89-4299-9191-4dc99804cb0e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "# Load the different models, trained on different datasets\n",
        "tokenizer_1 = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
        "model_1 = AutoModelForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
        "\n",
        "tokenizer_2 = AutoTokenizer.from_pretrained(\"aychang/roberta-base-imdb\")\n",
        "model_2 = AutoModelForSequenceClassification.from_pretrained(\"aychang/roberta-base-imdb\")\n",
        "\n",
        "tokenizer_3 = AutoTokenizer.from_pretrained(\"siebert/sentiment-roberta-large-english\")\n",
        "model_3 = AutoModelForSequenceClassification.from_pretrained(\"siebert/sentiment-roberta-large-english\")\n",
        "\n",
        "tokenizer_4 = AutoTokenizer.from_pretrained(\"lxyuan/distilbert-base-multilingual-cased-sentiments-student\")\n",
        "model_4 = AutoModelForSequenceClassification.from_pretrained(\"lxyuan/distilbert-base-multilingual-cased-sentiments-student\")\n",
        "\n",
        "tokenizer_5 = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-xlm-roberta-base-sentiment-multilingual\")\n",
        "model_5 = AutoModelForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-xlm-roberta-base-sentiment-multilingual\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ehms97AETM2F",
        "outputId": "fec95892-cde1-4342-8b42-2f641fa56ab0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "XLMRobertaForSequenceClassification(\n",
              "  (roberta): XLMRobertaModel(\n",
              "    (embeddings): XLMRobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(250002, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): XLMRobertaEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x XLMRobertaLayer(\n",
              "          (attention): XLMRobertaAttention(\n",
              "            (self): XLMRobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): XLMRobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): XLMRobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): XLMRobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (classifier): XLMRobertaClassificationHead(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (out_proj): Linear(in_features=768, out_features=3, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Move the models to GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_1.to(device)\n",
        "model_2.to(device)\n",
        "model_3.to(device)\n",
        "model_4.to(device)\n",
        "model_5.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "wYBNs5VlTM2F"
      },
      "outputs": [],
      "source": [
        "# Initialize the pipelines\n",
        "classifier_1 = pipeline('sentiment-analysis', model=model_1, tokenizer=tokenizer_1)\n",
        "classifier_2 = pipeline('sentiment-analysis', model=model_2, tokenizer=tokenizer_2)\n",
        "classifier_3 = pipeline('sentiment-analysis', model=model_3, tokenizer=tokenizer_3)\n",
        "classifier_4 = pipeline('sentiment-analysis', model=model_4, tokenizer=tokenizer_4)\n",
        "classifier_5 = pipeline('sentiment-analysis', model=model_5, tokenizer=tokenizer_5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o2sgMXjfTM2F"
      },
      "outputs": [],
      "source": [
        "# Predict sentiment labels for each classifier\n",
        "predictions_1 = classifier_1(comments)\n",
        "predictions_2 = classifier_2(comments)\n",
        "predictions_3 = classifier_3(comments)\n",
        "predictions_4 = classifier_4(comments)\n",
        "predictions_5 = classifier_5(comments)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CUuuabcfTM2F"
      },
      "outputs": [],
      "source": [
        "# Extract the scores from the predictions\n",
        "scores_1 = [prediction['score'] for prediction in predictions_1]\n",
        "scores_2 = [prediction['score'] for prediction in predictions_2]\n",
        "scores_3 = [prediction['score'] for prediction in predictions_3]\n",
        "scores_4 = [prediction['score'] for prediction in predictions_4]\n",
        "scores_5 = [prediction['score'] for prediction in predictions_5]\n",
        "# Extract the labels from the predictions\n",
        "labels_1 = [prediction['label'] for prediction in predictions_1]\n",
        "labels_2 = [prediction['label'] for prediction in predictions_2]\n",
        "labels_3 = [prediction['label'] for prediction in predictions_3]\n",
        "labels_4 = [prediction['label'] for prediction in predictions_4]\n",
        "labels_5 = [prediction['label'] for prediction in predictions_5]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JvhXo3aZxWMe"
      },
      "outputs": [],
      "source": [
        "# Extract the unique labels\n",
        "unique_labels_1 = set(labels_1)\n",
        "print(unique_labels_1)\n",
        "unique_labels_2 = set(labels_2)\n",
        "print(unique_labels_2)\n",
        "unique_labels_3 = set(labels_3)\n",
        "print(unique_labels_3)\n",
        "unique_labels_4 = set(labels_4)\n",
        "print(unique_labels_4)\n",
        "unique_labels_5 = set(labels_5)\n",
        "print(unique_labels_5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AV-LPEA7TM2F"
      },
      "outputs": [],
      "source": [
        "# Set up the right labels for the different models\n",
        "# We want to transform all labels to the same format ; all should be numbers where 0 is negative, 1 is neutral and 2 is positive\n",
        "# Model 1 gives Negative, Neutral, Positive as labels, so we will transform them to 0, 1, 2\n",
        "labels_1 = [0 if label == 'negative' else 1 if label == 'neutral' else 2 for label in labels_1]\n",
        "# Model 2 gives neg and pos as labels, so we will transform them to 0, 2\n",
        "labels_2 = [0 if label == 'neg' else 2 for label in labels_2]\n",
        "# Model 3 gives only POSITIVE, NEGATIVE as labels, so we will transform them to 0, 2\n",
        "labels_3 = [0 if label == 'NEGATIVE' else 2 for label in labels_3]\n",
        "# Model 4 gives negative, neutral, positive as labels, so we will transform them to 0,1,2\n",
        "labels_4 = [0 if label == 'negative' else 1 if label == 'neutral' else 2 for label in labels_4]\n",
        "# Model 5 gives negative, neutral, positive as labels, so we will transform them to 0,1,2\n",
        "labels_5 = [0 if label == 'negative' else 1 if label == 'neutral' else 2 for label in labels_5]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-6N6-YATM2G"
      },
      "outputs": [],
      "source": [
        "# Only keep comments with a confidence score of above 0.80\n",
        "conf_score = 0.80\n",
        "\n",
        "high_confidence_comments_1 = []\n",
        "high_confidence_comments_2 = []\n",
        "high_confidence_comments_3 = []\n",
        "high_confidence_comments_4 = []\n",
        "high_confidence_comments_5 = []\n",
        "\n",
        "high_confidence_comments_1_original = []\n",
        "high_confidence_comments_2_original = []\n",
        "high_confidence_comments_3_original = []\n",
        "high_confidence_comments_4_original = []\n",
        "high_confidence_comments_5_original = []\n",
        "\n",
        "high_confidence_predictions_1 = []\n",
        "high_confidence_predictions_2 = []\n",
        "high_confidence_predictions_3 = []\n",
        "high_confidence_predictions_4 = []\n",
        "high_confidence_predictions_5 = []\n",
        "\n",
        "high_confidence_scores_1 = []\n",
        "high_confidence_scores_2 = []\n",
        "high_confidence_scores_3 = []\n",
        "high_confidence_scores_4 = []\n",
        "high_confidence_scores_5 = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eI4G_zS-TM2G"
      },
      "outputs": [],
      "source": [
        "# For model 1 :\n",
        "for i in range(len(scores_1)):\n",
        "    if scores_1[i] > conf_score:\n",
        "        high_confidence_predictions_1.append(labels_1[i])\n",
        "        high_confidence_comments_1.append(comments[i])\n",
        "        high_confidence_comments_1_original.append(comments_original[i])\n",
        "        high_confidence_scores_1.append(scores_1[i])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9kCzTCgyTM2G"
      },
      "outputs": [],
      "source": [
        "# For model 2 :\n",
        "for i in range(len(scores_2)):\n",
        "    if scores_2[i] > conf_score:\n",
        "        high_confidence_predictions_2.append(labels_2[i])\n",
        "        high_confidence_comments_2.append(comments[i])\n",
        "        high_confidence_comments_2_original.append(comments_original[i])\n",
        "        high_confidence_scores_2.append(scores_2[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nVDY6PY9TM2G"
      },
      "outputs": [],
      "source": [
        "# For model 3 :\n",
        "for i in range(len(scores_3)):\n",
        "    if scores_3[i] > conf_score:\n",
        "        high_confidence_predictions_3.append(labels_3[i])\n",
        "        high_confidence_comments_3.append(comments[i])\n",
        "        high_confidence_comments_3_original.append(comments_original[i])\n",
        "        high_confidence_scores_3.append(scores_3[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ttLHC9W8vR77"
      },
      "outputs": [],
      "source": [
        "# For model 4 :\n",
        "for i in range(len(scores_4)):\n",
        "    if scores_4[i] > conf_score:\n",
        "        high_confidence_predictions_4.append(labels_4[i])\n",
        "        high_confidence_comments_4.append(comments[i])\n",
        "        high_confidence_comments_4_original.append(comments_original[i])\n",
        "        high_confidence_scores_4.append(scores_4[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_nDbwYt1vR77"
      },
      "outputs": [],
      "source": [
        "# For model 5 :\n",
        "for i in range(len(scores_5)):\n",
        "    if scores_5[i] > conf_score:\n",
        "        high_confidence_predictions_5.append(labels_5[i])\n",
        "        high_confidence_comments_5.append(comments[i])\n",
        "        high_confidence_comments_5_original.append(comments_original[i])\n",
        "        high_confidence_scores_5.append(scores_5[i])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0tDaA__akEjJ"
      },
      "outputs": [],
      "source": [
        "# Combining all comments and their respective labels\n",
        "combined_comments = high_confidence_comments_1 + high_confidence_comments_2 + high_confidence_comments_3 + high_confidence_comments_4 + high_confidence_comments_5\n",
        "combined_comments_original = high_confidence_comments_1_original + high_confidence_comments_2_original + high_confidence_comments_3_original + high_confidence_comments_4_original + high_confidence_comments_5_original\n",
        "combined_predictions = high_confidence_predictions_1 + high_confidence_predictions_2 + high_confidence_predictions_3 + high_confidence_predictions_4 + high_confidence_predictions_5\n",
        "combined_scores = high_confidence_scores_1 + high_confidence_scores_2 + high_confidence_scores_3 + high_confidence_scores_4 + high_confidence_scores_5\n",
        "\n",
        "# Counting the occurrence of each comment\n",
        "comment_counter = Counter(combined_comments)\n",
        "comment_original_counter = Counter(combined_comments_original)\n",
        "\n",
        "\n",
        "\n",
        "# Filtering comments that appear in at least three models with high confidence score\n",
        "filtered_comments = {comment for comment, count in comment_counter.items() if count >= 3}\n",
        "filtered_comments_original = {comment for comment, count in comment_original_counter.items() if count >= 3}\n",
        "\n",
        "\n",
        "\n",
        "# Dictionary to keep track of labels for each comment\n",
        "comment_labels = defaultdict(list)\n",
        "comment_original_labels = defaultdict(list)\n",
        "\n",
        "# Dictionary to keep track of scores for each comment\n",
        "comment_scores = defaultdict(list)\n",
        "comment_original_scores = defaultdict(list)\n",
        "\n",
        "# Populate the dictionary with labels and scores for each comment\n",
        "for comment, score, label in zip(combined_comments, combined_scores, combined_predictions):\n",
        "    if comment in filtered_comments:\n",
        "        comment_labels[comment].append(label)\n",
        "        comment_scores[comment].append((score, label))\n",
        "\n",
        "# Also for the original ones\n",
        "for comment, score, label in zip(combined_comments_original, combined_scores, combined_predictions):\n",
        "    if comment in filtered_comments_original:\n",
        "        comment_original_labels[comment].append(label)\n",
        "        comment_original_scores[comment].append((score, label))\n",
        "\n",
        "\n",
        "# Keep only labels that appear at least three times for each comment\n",
        "final_comments = []\n",
        "final_labels = []\n",
        "final_scores = []\n",
        "for comment, labels in comment_labels.items():\n",
        "    label_count = Counter(labels)\n",
        "    filtered_labels = [label for label, count in label_count.items() if count >= 3]\n",
        "    if filtered_labels:\n",
        "        final_comments.append(comment)\n",
        "        final_labels.append(filtered_labels)\n",
        "        # Also store the mean score of the models that provided the label gathered in filtered labels\n",
        "        final_scores.append(np.mean([x[0] for x in comment_scores[comment] if x[1] in filtered_labels]))\n",
        "\n",
        "\n",
        "\n",
        "final_comments_original = []\n",
        "final_labels_original = []\n",
        "final_scores_original = []\n",
        "for comment, labels in comment_original_labels.items():\n",
        "    label_count = Counter(labels)\n",
        "    filtered_labels = [label for label, count in label_count.items() if count >= 3]\n",
        "    if filtered_labels:\n",
        "        final_comments_original.append(comment)\n",
        "        final_labels_original.append(filtered_labels)\n",
        "        final_scores_original.append(np.mean([x[0] for x in comment_original_scores[comment] if x[1] in filtered_labels]))\n",
        "\n",
        "\n",
        "\n",
        "# The labels are all stored in lists, so we need to flatten them\n",
        "final_labels = [label for labels in final_labels for label in labels]\n",
        "final_labels_original = [label for labels in final_labels_original for label in labels]\n",
        "\n",
        "print(\"Filtered Comments:\", final_comments)\n",
        "print(\"Respective Labels:\", final_labels)\n",
        "print(\"Respective mean scores:\" , final_scores)\n",
        "\n",
        "\n",
        "assert len(final_comments) == len(final_labels) == len(final_scores), \"error\"\n",
        "\n",
        "\n",
        "\n",
        "# Check how many predictions we have in the respective classes\n",
        "print(\"We have \", final_labels.count(0), \" negative predictions.\")\n",
        "print(\"We have \", final_labels.count(1), \" neutral predictions.\")\n",
        "print(\"We have \", final_labels.count(2), \" positive predictions.\")\n",
        "\n",
        "# Seeing the lack of neutral predictions, we will omit this class for further analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wk-qgjn0TM2G"
      },
      "outputs": [],
      "source": [
        "# Check how many comments are left after filtering by confidence score\n",
        "print(\"We have \", len(final_comments), \" comments left after filtering by confidence score \" , conf_score , \" .\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rSEfW7GtTM2G"
      },
      "outputs": [],
      "source": [
        "# Save to csv the comments and their label\n",
        "high_confidence_comments_df = pd.DataFrame(final_comments, columns=['Comment'])\n",
        "high_confidence_comments_df['Label'] = final_labels\n",
        "high_confidence_comments_df['Score'] = final_scores\n",
        "high_confidence_comments_df.to_csv(path + \"High_Confidence_Comments_English_Split_{}.csv\".format(SPLIT))\n",
        "# Save original ones aswell\n",
        "high_confidence_comments_original_df = pd.DataFrame(final_comments_original, columns=['Comment'])\n",
        "high_confidence_comments_original_df['Label'] = final_labels_original\n",
        "high_confidence_comments_original_df['Score'] = final_scores_original\n",
        "high_confidence_comments_original_df.to_csv(path + \"High_Confidence_Comments_Original_English_Split_{}.csv\".format(SPLIT))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EoQ1z5-sv67"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BP3TjTx-TM2G"
      },
      "outputs": [],
      "source": [
        "# Download the file to your local machine (from google colab)\n",
        "\n",
        "files.download(path + \"High_Confidence_Comments_English_Split_{}.csv\".format(SPLIT))\n",
        "files.download(path + \"High_Confidence_Comments_Original_English_Split_{}.csv\".format(SPLIT))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
